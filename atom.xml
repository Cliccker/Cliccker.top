<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>不言物语</title>
  
  <subtitle>无远弗届</subtitle>
  <link href="https://cliccker.top/atom.xml" rel="self"/>
  
  <link href="https://cliccker.top/"/>
  <updated>2021-03-20T12:43:46.441Z</updated>
  <id>https://cliccker.top/</id>
  
  <author>
    <name>Hank</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>读文章—— An autonomous debating system</title>
    <link href="https://cliccker.top/post/ac95.html"/>
    <id>https://cliccker.top/post/ac95.html</id>
    <published>2021-03-19T02:22:16.000Z</published>
    <updated>2021-03-20T12:43:46.441Z</updated>
    
    <content type="html"><![CDATA[<p>“Hi Siri，打开美团外卖。” “你都这么胖了还吃外卖。”</p><p>“小爱同学，我想吃麻辣烫。” “我觉得你应该吃点更健康的食物。”</p><p>“Hi Siri，打开美团外卖。” “好的。”</p><p>“小爱同学，我想吃麻辣烫。” “好的。”</p><p>在我们的日常生活中，智能语音助手应该时时刻刻站在我们的角度思考，无条件的遵从用户的命令，帮助人们完成任务。然而不知道大家有没有思考过，只会遵从你命令的语音助手真的智能吗？最近一期Nature的封面文章<a href="https://www.nature.com/articles/s41586-021-03215-w" target="_blank" rel="noopener">An autonomous debating system</a>告诉我们，对抗性的交流，比如<strong>争论、辩论</strong>也是人类智能的一个重要组成部分，覆盖了人类众多的日常活动。作者在文章中展示了<code>Project Debater</code>——一个会“抬杠”的人工智能。</p><h2 id="关于Project-Debater"><a href="#关于Project-Debater" class="headerlink" title="关于Project Debater"></a>关于Project Debater</h2><p>首先因为文章涉及到辩论，我们先了解一下辩论中最主要的的两个专业名词：</p><blockquote><p><strong>动议</strong>Motion：也就是论题，你必须根据你队伍的立场来决定是支持还是反对。</p><p><strong>论点</strong>Argument：每个队伍都需要对动议提出直接的论点，论点包括观点，支撑该观点的证据以及将两者关联起来的分析过程，其结构更像是一篇文章后者一个段落。</p><p>​                                                                                                                                        <em><a href="https://www.asf.edu.mx/learning/events/asomex-debate-tournament/debating" target="_blank" rel="noopener">来源</a></em></p></blockquote><p>文章介绍了Project Debater是IBM在2012年启动的一个<a href="https://www.research.ibm.com/artificial-intelligence/project-debater/" target="_blank" rel="noopener">研究项目</a>，最终的目标是和一个冠军辩手进行现场辩论。这也可以说是一场公开的实验，我们一起看看这个实验的形式吧：</p><ol><li><p>确定辩论动议（Debate Motion），之后双方都可以准备15分钟。</p></li><li><p>双方做三段陈述，具体可以看图：</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210319154748.png" alt="陈述规则"></p></li><li><p>观众会在陈述之前和之后分别给正反方投票，能够拉到更多选票的人被视为获胜者。我不懂辩论的规则，我想应该是比谁更能过够改变观众的想法，就是<code>正→反</code>和<code>反→正</code>的数量对比。</p></li></ol><p>值得注意的是：</p><ul><li>人类选手不是随随便便选的，是曾经获世界大学生辩论赛冠军的选手；</li><li>这个实验的“动议”没有被用来构建Project Debater的训练数据；</li><li>这篇文章的重点是描述PD在一系列广泛话题上的表现，不局限于这次比赛。</li></ul><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>一场辩论中AI要完成的任务非常多，端到端的模型在这个场景里就不适用了，那PD是如何应付这些任务的呢？</p><p>文章提到的解决方法是将问题分解为并行执行的模块化具体任务，具体可以分为四个模块：</p><ol><li>论点挖掘（Argument Mining）</li><li>论点知识库（Argument Knowledge Base)</li><li>反驳点（Argument Rebuttal)</li><li>组织辩论语言（Debate Construction）</li></ol><p>下面分别介绍一下这四个模块</p><h3 id="论点挖掘"><a href="#论点挖掘" class="headerlink" title="论点挖掘"></a>论点挖掘</h3><p>该模型分成两个阶段，线下和线上：</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210320174335.png" alt="论点挖掘" style="zoom:67%;" /><p>对流程图的说明：</p><ol><li>声明（Claim）指对动议有<strong>明确立场</strong>的简明陈述；证据（Evidence）指能<strong>明确支撑或者反驳</strong>动议的一个简单句，用来证明声明是否真实。</li><li>检索算法是为了得到与论点相关的语句；</li><li>排序算法用的是神经网络模型，得到最相关的语句；</li><li>使用神经网络模型和基于知识的方法对语句的立场进行分类；</li><li>在论点挖掘阶段应用了<strong>主题扩展</strong>的组件，为的是寻找更多与辩论相关的概念；</li><li>在挖掘支持自身论点的同时也会挖掘<strong>支持对方的论点</strong>，以推理出对方可能会用到的声明或者证据，以便及时做出反应。</li></ol><h3 id="论点知识库"><a href="#论点知识库" class="headerlink" title="论点知识库"></a>论点知识库</h3><p>构建论点知识库旨在掌握不同辩论之间的共性，比如有的辩论虽然动议不同，但都是在讨论废除或者保持某项法例，属于相同的主题。</p><p>AKB中的文本可以是手动构建的，也可以是由机器抽取然后人为编辑的，可以是论点，也可以是引用，甚至是一些修饰性的类比和推理，他们被分成了不同的<strong>主题类别</strong>。</p><p>当被提及一个新的动议时，系统通过<strong>特征提取器</strong>来确定与该动议相关的<strong>主题类别</strong>，与该类别相关的文本都可以被用到接下来的发言之中，系统会根据他们与动议在<strong>语义上的联系</strong>来选择最相关文本。</p><p>除了应对新动议，AKB还是反驳模块的重要部分。当系统确定了对方表达了某个原则性论点，可以借助AKB来将这个论点映射为反驳他们的反论点。AKB中还包含了许多辩论中常见的情感术语，比如“有害的”，借助基于模式的方法，系统可以反驳聚焦于这些术语的论点。</p><p>可以说论点知识库是PD的小抄，密密麻麻的记载了各种话术、各种主题的辩论案例。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;“Hi Siri，打开美团外卖。” “你都这么胖了还吃外卖。”&lt;/p&gt;
&lt;p&gt;“小爱同学，我想吃麻辣烫。” “我觉得你应该吃点更健康的食物。”&lt;/p&gt;
&lt;p&gt;“Hi Siri，打开美团外卖。” “好的。”&lt;/p&gt;
&lt;p&gt;“小爱同学，我想吃麻辣烫。” “好的。”&lt;/p&gt;
&lt;p</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="AI" scheme="https://cliccker.top/tags/AI/"/>
    
    <category term="辩论" scheme="https://cliccker.top/tags/%E8%BE%A9%E8%AE%BA/"/>
    
    <category term="读论文" scheme="https://cliccker.top/tags/%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>关于如何申请JetBrain的学生许可证</title>
    <link href="https://cliccker.top/post/37a5.html"/>
    <id>https://cliccker.top/post/37a5.html</id>
    <published>2021-03-17T12:31:36.000Z</published>
    <updated>2021-03-17T13:55:27.936Z</updated>
    
    <content type="html"><![CDATA[<p>Pycharm是一个强大专业的Python编辑器，分为专业版和社区版，相比于专业版，社区版阉割了很多功能，比如<code>Django</code>。身边的同学选择用别人分享的激活码来激活专业版，当然这样既不能稳定保持激活，也不合适！</p><p>前些天我发现JetBrain官方给学生和教师提供了激活专业版的渠道，覆盖了旗下所有的产品。</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210317205018.png" alt="激活后的效果"></p><p>分享一下我激活的步骤八：</p><ol><li><p>首先，<strong>最重要的一点</strong>是，你要确认你有教育邮箱，一般就是你们学校提供的以<code>edu.cn</code>结尾的邮箱，比如我们学校的邮箱是<code>@zjut.edu.cn</code>。</p></li><li><p>然后去<a href="https://www.jetbrains.com/" target="_blank" rel="noopener">JetBrain</a>注册一个账号，亲测这一步用不用上面的邮箱都可。然后你会收到JetBrain发来的确认邮件，这边的信息都可以随便填我这就不展示了</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210317205627.png" alt="在这里填你自己的邮箱"></p></li><li><p>注册成功之后，你会看到这个界面，显示你没有激活的License</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210317210102.png" alt="没有License" style="zoom:67%;" /><p>这时候点击左上角回到JetBrain主页，找到这个地方</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210317210407.png" alt="申请入口" style="zoom:67%;" /></li><li><p>选择适用于学生和教师的许可证，点击最下方的申请。</p></li><li><p>然后你会看到这样一个界面，要填写的内容很少，点击申请他就给你发确认邮件了。</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210317211018.png" alt="填写信息"></p></li><li><p>这一步也<strong>非常关键</strong>，当我满怀期待的打开我的邮箱时，却发现里头空空如也，又去翻了翻垃圾箱，也是什么都没有。原来我们学校的邮件系统，会直接拒收某些邮件:cry:，不是拒绝之后放到垃圾箱，是直接拒收！！！你想找到这个邮件的话，要打开拦截队列：</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210317213220.png" alt="拦截队列"></p><p>然后在这个里头找你被拦截的邮件</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210317213342.png" alt="这是我们的邮件网关"></p></li><li><p>完成这一步之后，你就可以拥有为期一年的许可证了，到期了也没关系，JetBrain会在到期前七天给你发邮件通知，只要你还是学生，你就能续命一年。</p></li><li><p>拥有许可证的你，可以在激活软件的时候选择登入JetBrain账号，名正言顺的拥有专业版的Pycharm。当然你也可以试着下载Toolbox，便于下载JetBrain全家桶！</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Pycharm是一个强大专业的Python编辑器，分为专业版和社区版，相比于专业版，社区版阉割了很多功能，比如&lt;code&gt;Django&lt;/code&gt;。身边的同学选择用别人分享的激活码来激活专业版，当然这样既不能稳定保持激活，也不合适！&lt;/p&gt;
&lt;p&gt;前些天我发现JetBra</summary>
      
    
    
    
    <category term="技巧" scheme="https://cliccker.top/categories/%E6%8A%80%E5%B7%A7/"/>
    
    
    <category term="JetBrain" scheme="https://cliccker.top/tags/JetBrain/"/>
    
    <category term="Pycharm" scheme="https://cliccker.top/tags/Pycharm/"/>
    
    <category term="Python" scheme="https://cliccker.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>读论文—实例与本体概念联合嵌入知识库的通用表示学习</title>
    <link href="https://cliccker.top/post/ced9.html"/>
    <id>https://cliccker.top/post/ced9.html</id>
    <published>2021-03-17T05:30:00.000Z</published>
    <updated>2021-03-17T05:32:21.998Z</updated>
    
    <content type="html"><![CDATA[<p><em><strong>标题：</strong>Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts</em></p><p><em><strong>来源：</strong>KDD ‘19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em></p><p><em><strong>地址：</strong></em><a href="https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/10.1145/3292500.3330838">https://dl.acm.org/doi/10.1145/3292500.3330838</a></p><p><strong><em>代码地址：</em></strong><a href="https://link.zhihu.com/?target=https%3A//github.com/JunhengH/joie-kdd19">https://github.com/JunhengH/joie-kdd19</a></p><p>许多大型的<strong>知识库</strong>都<strong>同时</strong>表示知识图谱的两个视图，包括用来表示摘要或者常识的本体视图，和用来表示从本体中提取出来的特殊实体的实例视图。但是没有将两个视图<strong>单独</strong>表示出来的<strong>知识嵌入模型</strong>。该研究提出了two-view KG嵌入模型JOIE，目的是<strong>为了实现更好的知识嵌入</strong>以及<strong>支持依赖于多视图知识的新应用程序</strong>。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><img src="https://pic4.zhimg.com/v2-43423ec62e25059de7eadad7e7b2137f_b.jpg" alt=""></p><p>图1 双视角知识库示例。本体视图中橙色虚线表示层次元关系，黑色为规则元关系。</p><p>现存的知识图谱都<strong>可以</strong>分成如图1所示的两个视图</p><ol><li>实例视图的知识图谱，例如(“Barack Obama”,“isPoliticianOf ”, “United States”)</li><li>本体视图的知识图谱，例如(“polication”, “is leader of ”, “city”)</li></ol><p>同时，实例与本体视图之间由<strong>视图间(cross-view)</strong>关系来连接。</p><p>近年来对<strong>知识图谱的嵌入模型</strong>也有很多研究，但现有的模型都只针对其中<strong>一种视图</strong>进行设计。如果将双视图引入到<strong>知识表示模型</strong>中，可以有如下两点优势：</p><ol><li>实例嵌入为其相应的本体上的概念提供了详细而丰富的信息。例如，通过观察多个音乐家<strong>个体（instance）</strong>的嵌入，很大程度上可以确定其对应的<strong>概念（concept）</strong>“音乐家”的嵌入。</li><li>概念嵌入对其实例进行了<strong>高层次的总结</strong>，当一个实例很少被观察到时，这将会变得非常有帮助的，例如对于某个在<strong>实例视图</strong>中几乎没有关系的音乐家，我们仍然可以知道他/她在实例嵌入空间中的粗略位置，因为他/她不会离其他音乐家太远。</li></ol><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><strong>Challenge：</strong></p><ol><li>实例与概念、关系与元关系之间虽然不相交，但是存在语义上的联系，且两者做映射非常复杂。</li><li>现有的视图间关系往往不足以覆盖大量的实体，导致没有足够的信息区对齐两个视图，同时也限制了视图间发现新关系的能力</li><li>两种视图的规模和拓扑结构也有很大不同，其中本体视图通常是稀疏的，提供较少类型的关系，并形成分层的子结构，而实例视图则更大并且具有更多的关系类型。</li></ol><p><strong>Solution：</strong></p><p><img src="https://pic3.zhimg.com/v2-d5f11fd46aaeb101821c07aa1ebcdd42_b.jpg" alt=""></p><p>图2 JOIE模型结构</p><h2 id="建模"><a href="#建模" class="headerlink" title="建模"></a><strong>建模</strong></h2><p><img src="https://pic4.zhimg.com/v2-a381eee47a35bcedffe4060a2090526b_b.jpg" alt=""></p><p>图3 跨视图关联模型从跨视图链接（绿色“类别”框中的虚线箭头）学习嵌入。默认的视图内模型从每个视图中的三元组（灰色框）中学习嵌入。层次结构感知的视图内模型对在本体（或“层次结构”梯形）中形成层次结构的元关系事实进行建模</p><h3 id="跨视图关联模型"><a href="#跨视图关联模型" class="headerlink" title="跨视图关联模型"></a>跨视图关联模型</h3><ul><li>Cross-view Grouping (CG)——跨视图分组技术</li></ul><p><img src="https://pic1.zhimg.com/v2-cc9ef630dba6902e9175290a8983700c_b.jpg" alt=""></p><p>图4 跨视图分组技术</p><p>如图4所示，跨视图分组技术会假设本体视图的知识图谱和实例视图的知识图谱能够<strong>嵌入到同一空间中</strong>，并且“迫使”属于概念c的实例e在表示空间上<strong>靠近</strong>概念c。以此定义学习的loss为：</p><p><img src="https://www.zhihu.com/equation?tex=J_%7B%5Cmathrm%7BCross%7D%7D%5E%7B%5Cmathrm%7BCG%7D%7D%3D%5Cfrac%7B1%7D%7B%7C%5Cmathcal%7BS%7D%7C%7D+%5Csum_%7B%28e%2C+c%29+%5Cin+%5Cmathcal%7BS%7D%7D%5Cleft%5B%5C%7C%5Cmathrm%7Bc%7D-%5Cmathrm%7Be%7D%5C%7C_%7B2%7D-%5Cgamma%5E%7B%5Cmathrm%7BCG%7D%7D%5Cright%5D_%7B%2B%7D" alt="[公式]"> （1）</p><p>式中S表示具有is_A关系的实例与概念关系对； <img src="https://www.zhihu.com/equation?tex=%5Cgamma%5E%7B%5Cmathrm%7BCG%7D%7D" alt="[公式]"> 表示训练的超参数，也是图4中圆的半径；<img src="https://www.zhihu.com/equation?tex=%5Bx%5D_%7B%2B%7D+%3D%3E+max%5B+x%2C0%5D" alt="[公式]"> 。式（1）表示如果e出离了c圆之外，就会产生惩罚。</p><ul><li>Cross-view Transformation (CT)——跨视图转换技术</li></ul><p><img src="https://pic2.zhimg.com/v2-db4b6e544b5da0516ae4c9d15c9c1299_b.jpg" alt=""></p><p>图5 跨视图转换技术</p><p>和CG不同的是，CT允许两个视图<strong>彼此完全不相同</strong>，通过转换之后将两个视图<strong>对齐</strong>在一起。也就是说，转换之后，一个实例的嵌入将会被放置在本体视图中，且<strong>靠近</strong>该实例所属的概念嵌入。即</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bc%7D+%5Cleftarrow+f_%7B%5Cmathrm%7BCT%7D%7D%28%5Cmathbf%7Be%7D%29%2C+%5Cforall%28e%2C+c%29+%5Cin+%5Cmathcal%7BS%7D" alt="[公式]"> （2）</p><p>这里的 <img src="https://www.zhihu.com/equation?tex=f_%7B%5Cmathrm%7BCT%7D%7D%28%5Cmathbf%7Be%7D%29%3D%5Csigma%5Cleft%28%5Cmathbf%7BW%7D_%7B%5Cmathrm%7Bct%7D%7D+%5Ccdot+%5Cmathbf%7Be%7D%2B%5Cmathbf%7Bb%7D_%7B%5Cmathrm%7Bct%7D%7D%5Cright%29" alt="[公式]"> 是一个非线性的<strong>仿射变换</strong><a href="#ref_1">[1]</a>，由此定义loss</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bc%7D+J_%7B%5Cmathrm%7BCross%7D%7D%5E%7B%5Cmathrm%7BCT%7D%7D%3D%5Cfrac%7B1%7D%7B%7CS%7C%7D+%5Csum_%7B%28e%2C+c%29+%5Cin+S%5Cwedge%5Cleft%28e%2C+c%5E%7B%5Cprime%7D%5Cright%29+%5Cnotin+S%7D%5Cleft%5B%5Cgamma%5E%7B%5Cmathrm%7BCT%7D%7D%2B%5Cleft%5C%7C%5Cmathbf%7Bc%7D-f_%7B%5Cmathrm%7BCT%7D%7D%28%5Cmathbf%7Be%7D%29%5Cright%5C%7C_%7B2%7D-%5Cleft%5C%7C%5Cmathbf%7Bc%7D%5E%7B%5Cprime%7D-f_%7B%5Cmathrm%7BCT%7D%7D%28%5Cmathbf%7Be%7D%29%5Cright%5C%7C_%7B2%7D%5Cright%5D_%7B%2B%7D+%5C%5C++%5Cend%7Barray%7D" alt="[公式]"></p><p>(3)</p><p><strong>视图内嵌入模型</strong></p><p>视图内模型的目的是在两个嵌入空间中分别保留KB的每个视图中的原始结构信息。由于实例视图中的关系和本体视图中的元关系的语义含义不同，因此有助于为每个视图提供单独的处理方式，而不是将它们组合为单个表示模式，从而提高了性能。文中提供了两种嵌入模型。</p><ul><li>Default Intra-view Model——默认内部视图模型</li></ul><p>即通过现有的知识图谱表示模型来进行学习，包括TransE,DistMult,HolE，其得分函数如(4)</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+f_%7B%5Ctext+%7BTransE+%7D%7D%28%5Cmathbf%7Bh%7D%2C+%5Cmathbf%7Br%7D%2C+%5Cmathbf%7Bt%7D%29+%26%3D-%7C%7C+%5Cmathbf%7Bh%7D%2B%5Cmathbf%7Br%7D-%5Cmathbf%7Bt%7D+%5C%7C_%7B2%7D+%5C%5C+f_%7B%5Ctext+%7BMult+%7D%7D%28%5Cmathbf%7Bh%7D%2C+%5Cmathbf%7Br%7D%2C+%5Cmathbf%7Bt%7D%29+%26%3D%28%5Cmathbf%7Bh%7D+%5Ccirc+%5Cmathbf%7Bt%7D%29+%5Ccdot+%5Cmathbf%7Br%7D+%5C%5C+f_%7B%5Ctext+%7BHolE+%7D%7D%28%5Cmathbf%7Bh%7D%2C+%5Cmathbf%7Br%7D%2C+%5Cmathbf%7Bt%7D%29+%26%3D%28%5Cmathbf%7Bh%7D+%5Cstar+%5Cmathbf%7Bt%7D%29+%5Ccdot+%5Cmathbf%7Br%7D+%5Cend%7Baligned%7D" alt="[公式]"> (4)</p><p>优化目标即最小化所有三元组（包括实例间三元组和概念间三元组）的loss：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+J_%7B%5Ctext+%7BIntra+%7D%7D%5E%7BG%7D%3D%5Cfrac%7B1%7D%7B%7C%5Cmathcal%7BG%7D%7C%7D+%26+%5Csum_%7B%28h%2C+r%2C+t%29+%5Cin+%5Cmathcal%7BG%7D%5Cwedge%5Cleft%28h%5E%7B%5Cprime%7D%2C+r%2C+t%5E%7B%5Cprime%7D%5Cright%29+%5Cnotin+%5Cmathcal%7BG%7D%7D%5Cleft%5B%5Cgamma%5E%7B%5Cmathcal%7BG%7D%7D%2Bf%5Cleft%28%5Cmathbf%7Bh%7D%5E%7B%5Cprime%7D%2C+%5Cmathbf%7Br%7D%2C+%5Cmathbf%7Bt%7D%5E%7B%5Cprime%7D%5Cright%29-f%28%5Cmathbf%7Bh%7D%2C+%5Cmathbf%7Br%7D%2C+%5Cmathbf%7Bt%7D%29%5Cright%5D_%7B%2B%7D+%5C%5C+%26++%5Cend%7Baligned%7D" alt="[公式]"> (5)</p><p>其中 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88%5Cmathbf%7Bh%7D%5E%7B%5Cprime%7D%2C+%5Cmathbf%7Br%7D%2C+%5Cmathbf%7Bt%7D%5E%7B%5Cprime%7D%EF%BC%89" alt="[公式]"> 表示表示头尾被替换，不存在于图谱中的三元组。这个损失天然就适用于两个不同的视图，我们用两个视图的损失函数构建一个<strong>联合损失函数</strong>：</p><p><img src="https://www.zhihu.com/equation?tex=J_%7B%5Ctext+%7BIntra+%7D%7D%3DJ_%7B%5Ctext+%7BIntra+%7D%7D%5E%7BG_%7BI%7D%7D%2B%5Calpha_%7B1%7D+%5Ccdot+J_%7B%5Ctext+%7BIntra+%7D%7D%5E%7BG_%7BO%7D%7D" alt="[公式]"> （6）</p><p>式中的α1用来调节权重</p><ul><li>Hierarchy-Aware Intra-view Model for the Ontology. ——本体的层次感知视图内模型</li></ul><p>默认内部视图模型并不能对本体视图的概念间上下位关系进行很好的建模。作者提出了层次感知视图内模型，这一模型进一步区分了<strong>形成本体层级结构</strong>的元关系（如”subclass_of”和”is_a”)和视图内模型中<strong>常规</strong>的语义关系（如”related_to”)。</p><p>这里设计方法类似CT，即给定一个具备subclass_of关系的概念对 <img src="https://www.zhihu.com/equation?tex=%28c_l%2Cc_t%29" alt="[公式]"> ，认为存在非线形变换可以将<strong>粗粒度</strong>的概念（上位概念）映射为<strong>细粒度</strong>的概念（下位概念），其公式可以表示：</p><p><img src="https://www.zhihu.com/equation?tex=g_%7B%5Cmathrm%7BHA%7D%7D%5Cleft%28%5Cmathrm%7Bc%7D_%7Bh%7D%5Cright%29%3D%5Csigma%5Cleft%28%5Cmathrm%7BW%7D_%7B%5Cmathrm%7BHA%7D%7D+%5Ccdot+%5Cmathrm%7Bc%7D_%7Bl%7D%2B%5Cmathrm%7Bb%7D_%7B%5Cmathrm%7BHA%7D%7D%5Cright%29" alt="[公式]"> （7）</p><p>式中的<img src="https://www.zhihu.com/equation?tex=%7BW%7D_%7B%5Cmathrm%7BHA%7D%7D+%E3%80%81%7Bb%7D_%7B%5Cmathrm%7BHA%7D%7D+" alt="[公式]">都是训练的参数。由此定义的loss为</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+J_%7B%5Ctext+%7BIntra+%7D%7D%5E%7B%5Cmathrm%7BHA%7D%7D%3D%5Cfrac%7B1%7D%7B%7C%5Cmathcal%7BT%7D%7C%7D+%26+%5Csum_%7B%5Cleft%28c_%7Bl%7D%2C+c_%7Bh%7D%5Cright%29+%5Cin+%5Cmathcal%7BT%7D%5Cwedge%5Cleft%28c_%7Bl%7D%2C+c_%7Bh%7D%5E%7B%5Cprime%7D%5Cright%29+%5Cnotin+%5Cmathcal%7BT%7D%7D%5Cleft%5B%5Cgamma%5E%7B%5Cmathrm%7BHA%7D%7D%2B%5Cleft%5C%7C%5Cmathbf%7Bc%7D_%7Bh%7D-g%5Cleft%28%5Cmathbf%7Bc%7D_%7Bl%7D%5Cright%29%5Cright%5C%7C_%7B2%7D-%5Cleft%5C%7C%5Cmathbf%7Bc%7D_%7B%5Cmathbf%7Bh%7D%7D%5E%7B%5Cprime%7D-g%5Cleft%28%5Cmathbf%7Bc%7D_%7B%5Cmathbf%7Bl%7D%7D%5Cright%29%5Cright%5C%7C_%7B2%7D%5Cright%5D_%7B%2B%7D+%5C%5C+%26++%5Cend%7Baligned%7D" alt="[公式]"> （8）</p><p>再列出联合loss</p><p><img src="https://www.zhihu.com/equation?tex=J_%7B%5Ctext+%7BIntra+%7D%7D%3DJ_%7B%5Ctext+%7BIntra+%7D%7D%5E%7B%5Cmathcal%7BG%7D_%7BI%7D%7D%2B%5Calpha_%7B1%7D+%5Ccdot+J_%7B%5Ctext+%7BIntra+%7D%7D%5E%7B%5Cmathcal%7BG%7D+%5Ccirc%5Cbackslash+%5Cmathcal%7BT%7D%7D+%2B%5Calpha_%7B2%7D+%5Ccdot+J_%7B%5Ctext+%7BIntra+%7D%7D%5E%7B%5Cmathrm%7BHA%7D%7D" alt="[公式]"> （9）</p><p>式中 <img src="https://www.zhihu.com/equation?tex=J_%7B%5Ctext+%7BIntra+%7D%7D%5E%7BG+%5Ccirc%5Cbackslash+%5Cmathcal%7BT%7D%7D" alt="[公式]"> 表示使用<strong>默认内部视图模型</strong>再常规语义关系三元组上训练所得到的损失。<img src="https://www.zhihu.com/equation?tex=J_%7B%5Ctext+%7BIntra+%7D%7D%5E%7B%5Cmathrm%7BHA%7D%7D" alt="[公式]"> 表示在具有本体关系<strong>层次结构的元关系</strong>的三元组上进行了明确的训练之后得到的损失，这与等式6有很大不同。</p><h2 id="联合训练"><a href="#联合训练" class="headerlink" title="联合训练"></a>联合训练</h2><p><img src="https://www.zhihu.com/equation?tex=J%3DJ_%7B%5Ctext+%7BIntra+%7D%7D%2B%5Comega+%5Ccdot+J_%7B%5Ctext+%7BCross+%7D%7D" alt="[公式]"> （10）</p><p>式中ω是一个大于零的正参数，用来平衡两个J。</p><p>文中还讲到了他们的训练手法</p><blockquote><p>Instead of directly updating J, our implementation optimizes JIntra GI , JIntra GO and JCross alternately. In detail, we optimize θnew ← θold - η∇JIntra and θnew ← θold - (ωη)∇JCross in successive steps within one epoch.η is the learning rate, and ω differentiates between the learning rates for intra-view and cross-view losses.</p></blockquote><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li>数据集</li></ul><p>数据集方面，由于现有方法大多只关注对一个视图的知识建模，缺少融合两个视图特点的公开评测数据。所以作者从YAGO和DBpedia构建了两个更符合真实知识图谱结构的数据集YAGO26K-906和DB111K-174。数据集的相关信息统计如下<a href="#ref_2">[2]</a>：</p><p><img src="https://pic4.zhimg.com/v2-5c933cf75d82dce678c5df423c07ec23_b.jpg" alt=""></p><p>表1 数据集概览</p><p>主要完成了<strong>知识图谱补全</strong>和<strong>实体分类</strong>的两个任务，都显示了该模型有不错的性能。这里篇幅有限，就不给人家打广告了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="#ref_1_0">^</a>仿射变换 <a href="https://zh.wikipedia.org/wiki/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2</a></li><li><a href="#ref_2_0">^</a>机器之心 <a href="https://www.jiqizhixin.com/articles/2019-08-12-3" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-08-12-3</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;标题：&lt;/strong&gt;Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts&lt;/em&gt;</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="KB" scheme="https://cliccker.top/tags/KB/"/>
    
    <category term="嵌入学习" scheme="https://cliccker.top/tags/%E5%B5%8C%E5%85%A5%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用PyQT写小工具</title>
    <link href="https://cliccker.top/post/3515.html"/>
    <id>https://cliccker.top/post/3515.html</id>
    <published>2021-01-19T02:24:29.000Z</published>
    <updated>2021-01-19T04:27:55.297Z</updated>
    
    <content type="html"><![CDATA[<p>最近写的论文有手动建立案例的需求，一开始采用的方法是建一个json模板，然后在模板上改关键词，就像这样：</p><pre class=" language-json"><code class="language-json"><span class="token punctuation">{</span>    <span class="token property">"有限元分析案例"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">{</span>            <span class="token property">"产品信息"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                <span class="token punctuation">{</span>                    <span class="token property">"产品名称"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"分析对象"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"产品材料"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span>                <span class="token punctuation">}</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token property">"设计"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                <span class="token punctuation">{</span>                    <span class="token property">"设计尺寸"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"设计参数"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"补强条件"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span>                <span class="token punctuation">}</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token property">"工况"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                <span class="token punctuation">{</span>                    <span class="token property">"介质"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"环境条件"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"应力条件"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span>                <span class="token punctuation">}</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token property">"分析条件"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                <span class="token punctuation">{</span>                    <span class="token property">"材料属性"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"边界条件"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"接触类型"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"约束条件"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token property">"载荷类型"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                        <span class="token string">"none"</span>                    <span class="token punctuation">]</span>                <span class="token punctuation">}</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token property">"分析类型"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                <span class="token string">"none"</span>            <span class="token punctuation">]</span>        <span class="token punctuation">}</span>    <span class="token punctuation">]</span><span class="token punctuation">}</span></code></pre><p>但是这样做效率太低了，遂有了做一个小工具的想法。主要实现几个功能：</p><ul><li>信息通过文本框录入，是最基本的交互方式；</li><li>建立的案例以json格式保存；</li><li>要能够读取和修改案例，实现案例的管理（还没做😂）</li></ul><p>最后做出来这个样子的界面：</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210119103800.png" alt="小工具的主界面"></p><p>总结一下做这个的时候遇到的一些坑：</p><h3 id="1-Text与PlainText（小坑）"><a href="#1-Text与PlainText（小坑）" class="headerlink" title="1. Text与PlainText（小坑）"></a>1. Text与PlainText（小坑）</h3><p>QTdesigner中有两种文本框输入控件</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210119104145.png" alt="两种控件"></p><p>一开始我用了第一种<code>Text Edit</code>，手动输入文本没有问题，但是从别处复制粘贴过来的文本都是带格式的。再看这俩控件的图标，原来<code>Plain Text</code>意思是无论输入什么格式的文本，都会变成不带格式的普通文本。</p><h3 id="2-Layout和Frame（小坑）"><a href="#2-Layout和Frame（小坑）" class="headerlink" title="2. Layout和Frame（小坑）"></a>2. Layout和Frame（小坑）</h3><p>一开始写的时候标签和文本框都是手动对齐的😂</p><p>直到用上了<code>Layout</code>和<code>Frame</code>，这俩能让你的图形界面看起来更像一个图形界面。但是这俩有啥不同能，看看他们之间参数上的差异：</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210119105537.png" alt="Layout"></p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210119105653.png" alt="Frame"></p><p>简单来说，<code>Layout</code>作为幕后黑手，规定了整个图形界面的布局样式，我们看不到他。而<code>Frame</code>作为容器，我们可以通过改变他的样式来给<code>Frame</code>之间做区分。</p><h3 id="3-Table-Widget（大坑）"><a href="#3-Table-Widget（大坑）" class="headerlink" title="3. Table Widget（大坑）"></a>3. Table Widget（大坑）</h3><p>因为一个产品中可能会有好几种材料，这些材料要考虑的参数也不一样。所以要一条一条添加材料的信息。一开始的想法是，点一次按钮就往Frame里加一个这样的Widget：</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20210119111036.png" alt="材料信息Widget"></p><p>可这就带来了一个新的问题，别的文本框都能这样获取文本</p><pre class=" language-python"><code class="language-python">CaseName <span class="token operator">=</span> self<span class="token punctuation">.</span>CaseName<span class="token punctuation">.</span>toPlainText<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>但是这个Widget中文本框没有标签名，需要遍历整个Frame来获取文本。这很麻烦，效率很低。</p><p>然后我注意到了QTDesigner中的Table Widget，该控件更适合这种场景。但又遇到一个问题，如果我要初始化表格，需要一行一行的删除，正常的思路是从头删到尾，可怎么写都会出现，有一行没删掉或者多删了一行的问题。找了一下网上的方法，原来要从最后一行开始，写一个逆循环来删：</p><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">initTable</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        初始化材料表格        """</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>MaterialTable<span class="token punctuation">.</span>rowCount<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 删除新增的行</span>            self<span class="token punctuation">.</span>MaterialTable<span class="token punctuation">.</span>removeRow<span class="token punctuation">(</span>i<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>MaterialTable<span class="token punctuation">.</span>insertRow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近写的论文有手动建立案例的需求，一开始采用的方法是建一个json模板，然后在模板上改关键词，就像这样：&lt;/p&gt;
&lt;pre class=&quot; language-json&quot;&gt;&lt;code class=&quot;language-json&quot;&gt;&lt;span class=&quot;token punct</summary>
      
    
    
    
    
    <category term="pyqt5" scheme="https://cliccker.top/tags/pyqt5/"/>
    
    <category term="QTdesigner" scheme="https://cliccker.top/tags/QTdesigner/"/>
    
  </entry>
  
  <entry>
    <title>关于播客的思考</title>
    <link href="https://cliccker.top/post/721.html"/>
    <id>https://cliccker.top/post/721.html</id>
    <published>2020-11-08T07:38:00.000Z</published>
    <updated>2020-11-08T09:10:19.538Z</updated>
    
    <content type="html"><![CDATA[<img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20201108154303.png" alt="我的小宇宙收听记录" style="zoom: 33%;" /><p>一个多月前，开始用小宇宙。</p><p>我自己之前也做过播客，都已经是三年前的事了，我一个人写词，录音，剪辑。大概做了有二十多期。</p><p>我的听众来自我的朋友圈，当然我也在网易云的电台上更新，不过几乎没有人收听。</p><p>现在思考一下原因，大概是因为我的播客没有<strong>内核</strong>，或者说我选了一个很弱的“内核”——推荐我喜欢的音乐<br>——这其实是一个重情绪的内核。</p><p>让我产生做播客想法的，是在广播站的工作，帮人录音，剪辑音频。但是也不知道是人的天性还是其他原因，我总会有那样一种想法：他们做的还不如我呢。给很多主播录过，书评、影评占了大多数，偶尔还有人物传记。为数不多的对谈节目往往也是为了引出上述的内容：</p><p>“小明，最近有什么电影推荐吗？”</p><p>“啊，这你就问对人了。”</p><p>所以我开始思考新的形式，能让人沉浸的形式。由于是一个人做，不考虑对谈。左思右想还是做音乐吧，节目的构思是每天推荐一首钟意的音乐，配上自己写的评论或者小故事，由于是暑假，基本做到了每天更新。还在朋友圈挂起横幅大肆宣传，但是我猜真正听的人大概不超过五个。现在想，要是我自己也不愿意听，我不能从这个电台中获得<strong>情绪</strong>以外的任何东西。当我还在思考这么收听量+1时，已经有人在播客节目上演情景剧了（<a href="https://www.ximalaya.com/guangbojv/16612836/" target="_blank" rel="noopener">六里庄人民广播电台</a>）。</p><p>明白了内容创作有多么需要沉淀。</p><p><em>完</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20201108154303.png&quot; alt=&quot;我的小宇宙收听记录&quot; style=&quot;zoom: 33%;&quot; /&gt;

&lt;p&gt;一个多月前，开始用小宇宙。&lt;/p&gt;</summary>
      
    
    
    
    <category term="想法" scheme="https://cliccker.top/categories/%E6%83%B3%E6%B3%95/"/>
    
    
    <category term="随便写写" scheme="https://cliccker.top/tags/%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/"/>
    
  </entry>
  
  <entry>
    <title>读论文——Knowledge Graph Embedding Based Question Answering</title>
    <link href="https://cliccker.top/post/13817.html"/>
    <id>https://cliccker.top/post/13817.html</id>
    <published>2020-11-01T04:00:00.000Z</published>
    <updated>2020-11-09T07:50:48.207Z</updated>
    
    <content type="html"><![CDATA[<p>来源：WSDM ’19</p><p><a href="https://dl.acm.org/doi/10.1145/3289600.3290956" target="_blank" rel="noopener">论文地址</a></p><p><a href="https://github.com/xhuang31/KEQA_WSDM19" target="_blank" rel="noopener">github</a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><h3 id="目标领域"><a href="#目标领域" class="headerlink" title="目标领域"></a>目标领域</h3><p>对于终端用户（end user）来说，知识图谱（KG）的结构往往过于复杂，难以访问有价值的信息，基于知识图的问答（QA-KG）旨在解决这一问题，以让用户更高效的获取知识图谱中蕴含的讯息。举例说明：</p><ul><li><p>Q：Which Olympic was in Beijing?</p><p>QA-KG旨在确定这个问题相关的两个事实，用三元组的形式可以表示为</p></li><li><p>(Beijing,Olympics_participated_in,2008 Summer Olympics)</p></li></ul><h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><ol><li><p>谓语在自然语言中存在多种不同的表达方式。</p><p>例：谓语<em>person.nationality</em>可以表示为</p><ul><li>“what is … ’s nationality”</li><li>“which country is … from”</li><li>“where is … from”</li></ul><p>三种形式。</p></li><li><p>模糊或者不全的实体名会导致答案的数量过于庞大。</p><p>例：”How old is Obama?”</p><p>只包含了<em>Barak Obama​</em>这一实体的一部分。</p></li><li><p>用户提出问题的领域五花八门，没有哪一个知识图谱能够涵盖所有领域。</p></li></ol><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><ol><li>提出了一种以知识图谱嵌入为基础的问答系统框架（KEQA），将问题中的头部实体，谓语，尾部实体嵌入到向量空间，用以解决上述的三个问题。</li><li>提出了一种考虑知识图谱中结构和关系差异的联合距离度量方式。</li></ol><h2 id="KEQA"><a href="#KEQA" class="headerlink" title="KEQA"></a>KEQA</h2><h3 id="为什么使用KGE（知识图谱嵌入）技术？"><a href="#为什么使用KGE（知识图谱嵌入）技术？" class="headerlink" title="为什么使用KGE（知识图谱嵌入）技术？"></a>为什么使用KGE（知识图谱嵌入）技术？</h3><p>知识图嵌入目标用于学习KG中每个谓词/实体的低维矢量表示，使得原始关系在矢量中得到很好的保留。 这些学习的矢量表示可用于有效地完成各种下游应用。目前已经有了知识图谱补全、关系提取、推荐系统等方面的应用。</p><h3 id="KEQA的主要思想"><a href="#KEQA的主要思想" class="headerlink" title="KEQA的主要思想"></a>KEQA的主要思想</h3><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20201101155507.png" alt="图1 KEQA如何回答一个简单问题"></p><p>如图所示，图谱G被嵌入到了两个低维的向量空间中，包括谓语嵌入空间和实体嵌入空间。每个事实三元组$(h,l,t)$都有其向量表示$(e_h,p_l,e_t)$。给定一个问题Q，首先要学习到该问题的$e_h$、$p_l$，并且预测出$e_t$，组成<em>“预测事实”</em>，再通过<em>度量向量距离</em>寻找同预测事实最接近的<em>事实</em>，就能得到该问题可能的答案。</p><h3 id="知识图谱的嵌入"><a href="#知识图谱的嵌入" class="headerlink" title="知识图谱的嵌入"></a>知识图谱的嵌入</h3><p>文章简单描述了所采用的嵌入方法，其核心思想与TransE、TransR相同，都是通过最小化$e_t$和$f(e_h,p_l)$之间的距离来得到实体和关系的嵌入（函数$f(·)$是用来衡量三元组中向量关系的函数。TransE中$f(e_h,p_l)=e_h+p_l$），文章没有明确具体是哪一种方法。</p><h3 id="谓语和头部实体学习模型"><a href="#谓语和头部实体学习模型" class="headerlink" title="谓语和头部实体学习模型"></a>谓语和头部实体学习模型</h3><p>给定一个简单问题，目标是找到谓语向量空间中的一点来作为该问题的谓语向量$p_l$，在实体向量空间中找到一点作为头部实体向量$e_h$。该模型的核心思想是考虑不同问题中词的语序（order）和词的重要性（importance），为此文中引入了双向LSTM和注意力机制。关于词的重要性，文章中举例说明：当我们在学习谓语的向量时，实体就显得不那么重要了。</p><h4 id="谓语表示学习"><a href="#谓语表示学习" class="headerlink" title="谓语表示学习"></a>谓语表示学习</h4><p>谓语，只得是一句话中用来描述主语的部分，如John went home中的went home。</p><p>一般预测谓语的方法包括：</p><ol><li>学习基于语义解析的映射，再人为的标注语义；</li><li>为每个谓语做标注，转化成分类任务</li></ol><p>文章提出，用户提出问题的领域会非常广泛，以至于一个新问题的谓语可能与其他所有已有的谓语都不相同。</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20201102191453.png" alt="图2 谓语表示神经网络结构"></p><p>模型训练时的目标是：输入一个已知答案的简单问句，找到离$p_l$（标注）最近的$\hat p_l$（预测）</p><h4 id="头部实体表示学习及头部实体识别"><a href="#头部实体表示学习及头部实体识别" class="headerlink" title="头部实体表示学习及头部实体识别"></a>头部实体表示学习及头部实体识别</h4><p>神经网络的结构与谓语表示学习相同，不同点在于图谱中的实体数往往非常庞大，如果要将$\hat e_h$与每一个实体的$e_h$比较的话耗费会非常大，因此文章引入了一个<strong>头部实体检测模型HED</strong>，该模型的训练集是一系列<strong>已知事实</strong>的问题，其结构如图所示：</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20201102210212.png" alt="图3 头部实体检测模型"></p><p>可以观察到该网络与谓语表示学习网络的结构也十分相似，不同点在于没有Attention，同时输出结果$v_j$是一个二维的向量，其含义如图。很明显，该模型的训练目标是找到与已知事实中的头部实体名称相同的token。</p><p>利用训练过的HED模型，我们可以标记出问题中的实体名称，所有包含该实体名称的实体都将作为候选实体。</p><p>简单来说，这一步在做实体名称的模糊匹配，比如问题是“How old is Obama?”，HED识别出了实体名称”Obama”，包含该实体名称的实体包括”Barak Obama”,”Michelle Obama”,”Obama”等等，这些实体所属的三元组都将被列为候选三元组，相比之下，查找范围缩小了很多，也在一定程度上解决了<strong>实体名称不全</strong>的问题。但该方法仍有不足，因为KG会有很多实体是<strong>同名</strong>的。</p><h3 id="向量空间上的联合搜索"><a href="#向量空间上的联合搜索" class="headerlink" title="向量空间上的联合搜索"></a>向量空间上的联合搜索</h3><p>前面的部分能够得到一个问题谓语和头部实体的表示$\hat p_l和\hat e_h$，以及若干个候选三元组，最后的任务是在候选三元组中找到最符合条件（距离最近）的三元组，距离的计算公式如下：</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20201108135559.png" alt="联合距离计算公式"></p><p>第一行表示候选三元组与预测结果在空间上的距离，其中尾部三元组的向量表示$e_t$由$f(e_h,p_t)$求得，而不是直接选用候选三元组中的$e_t$，这样做是因为在图谱中可能会存在$e_h,p_l$相同但$e_t$不同的情况；</p><p>第四项表示尽量选择候选实体名称与标记出的$HED_{entity}$相同的三元组；</p><p>第五项表示尽量选择谓语被问题所提及的三元组。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20201104160911.png" alt="图4 数据集"></p><p>FB2M和FB5M都是Freebase的子集，包含大量结构化的数据。</p><p>SimpleQuestions包含了一万多个有相关事实的简单问题，且这些事实都存在于FB2M中</p><h2 id="感慨"><a href="#感慨" class="headerlink" title="感慨"></a>感慨</h2><p>我导说我这次报告不能说服他！是我真的没看懂吗😂</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;来源：WSDM ’19&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3289600.3290956&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="知识图谱" scheme="https://cliccker.top/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    <category term="知识嵌入" scheme="https://cliccker.top/tags/%E7%9F%A5%E8%AF%86%E5%B5%8C%E5%85%A5/"/>
    
    <category term="问答系统" scheme="https://cliccker.top/tags/%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>焦虑逃离计划</title>
    <link href="https://cliccker.top/post/19305.html"/>
    <id>https://cliccker.top/post/19305.html</id>
    <published>2020-10-09T07:26:54.000Z</published>
    <updated>2020-10-09T10:24:56.012Z</updated>
    
    <content type="html"><![CDATA[<p>采访了一下身边的“社会人”（拒绝使用社畜这种词！）小吴同学，了解到了她的焦虑。</p><p>“那个湖边上的外婆家，当初我来杭州找不到工作，就在那边坐了很久，那段日子真的很焦虑。”</p><p>“每天都在花钱住酒店，工作又没啥消息，都是小破公司。”</p><p>“感觉自己能力不够。”</p><p>“很着急。”</p><p>这是应届毕业生小吴的焦虑，可能是千千万万毕业生的焦虑。小吴说那段时间一直在欺负我取乐，包括但不限于掐我屁股，而我的印象里是我一直在陪着小吴，包括但不限于抱着流眼泪的她。她最后找到了工作，在北京，她很开心，还请我吃了饭。</p><p>“有你好很多”，小吴说。</p><p>“对未来的未知，不自信，不知道处于什么位置，该怎么前进。”</p><p>这是工作一年以后小吴的焦虑，上了一年多班的小吴开始物色下一份工作，一方面是因为现在的公司行将就木，另一方面是因为我俩相隔一千多公里她掐不到我的屁股。</p><p>“我最近好很多了，看书还是有用的”</p><p>看书成了她逃离焦虑的新方法，她开始看各种专业书籍，大牛著作，她开始更新自己尘封已久的公众号，放上自己的读书笔记。她开始听有意思的播客，我的小宇宙邀请码就是她给的。当然，小吴同学也没有停止在言语上骚扰我的屁股！</p><p>总结以下小吴同志的方针：</p><ol><li>找一个陪伴你的人</li><li>不断学习，不断进步</li></ol><p>至于我，我遵守两个方针：</p><ol><li>感到焦虑就逃回自己的舒适圈。躺在床上打开小吴送的（高亮）switch，最舒适的甜甜圈！</li><li>多和别人交流，最好是面对面交流，互相交换坏情绪，坏情绪就会被慢慢消耗掉。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;采访了一下身边的“社会人”（拒绝使用社畜这种词！）小吴同学，了解到了她的焦虑。&lt;/p&gt;
&lt;p&gt;“那个湖边上的外婆家，当初我来杭州找不到工作，就在那边坐了很久，那段日子真的很焦虑。”&lt;/p&gt;
&lt;p&gt;“每天都在花钱住酒店，工作又没啥消息，都是小破公司。”&lt;/p&gt;
&lt;p&gt;“感觉自</summary>
      
    
    
    
    <category term="想法" scheme="https://cliccker.top/categories/%E6%83%B3%E6%B3%95/"/>
    
    
    <category term="随笔" scheme="https://cliccker.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>读论文——graph2vec:图的分布式表示学习</title>
    <link href="https://cliccker.top/post/7820.html"/>
    <id>https://cliccker.top/post/7820.html</id>
    <published>2020-09-07T06:30:42.000Z</published>
    <updated>2020-09-09T13:17:30.485Z</updated>
    
    <content type="html"><![CDATA[<p>原论文：graph2vec: Learning Distributed Representations of Graphs</p><p>来源：MLG 2017 - 13th International Workshop on Mining and Learning with Graphs (MLG 2017)</p><p><a href="https://arxiv.org/abs/1707.05005" target="_blank" rel="noopener">论文地址</a></p><p><a href="https://github.com/benedekrozemberczki/graph2vec" target="_blank" rel="noopener">github地址</a></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>现如今的很多研究集中在如何表示图谱中子结构的分布式表示，如节点、子图等。但是在对图的分类和聚类这些知识图谱的分析任务中，如果采用现有的手段，我们就需要得到整个图谱的表示。因此在处理这些分析任务时，图核（Graph Kernel）方法更加有效。</p><blockquote><p>Graph Kernel 方法将机器学习中的核方法（Kernel Methods）拓展到了图结构数据上，是一类计算图与图之间相似度的方法。再利用图核方法比较两个图的相似度时，需要将两个图用一定方法（如最小路径，随机游走，小图等）分解成更小的子结构，再通过定义一个核函数来的到两个图的相似度（如计算两个图中相似的子结构个数）</p></blockquote><blockquote><p>为什么是Graph Kernels而不是用Graph Embedding？</p><p>因为后者将结构化数据降维到向量空间时，损失了大量结构化信息。而前者直接面向图结构的数据，保留了核函数高效计算的优点，又包含了结构化的信息。</p></blockquote><p>文章提出，这种方式有两种存在的问题：</p><ol><li><p>不能得到显式的图嵌入，不利于计算和学习。</p></li><li><p>“人为”定义的特征（路径，步伐）不具有概括性。</p><blockquote><p>这些“人为”的特征应用在大型数据集上时，会产生高维的，稀疏的，不光滑的表示。</p></blockquote></li></ol><h2 id="解决手段"><a href="#解决手段" class="headerlink" title="解决手段"></a>解决手段</h2><p>为了解决上述的问题，文章提出了将word2vec中的Skipgram模型引入到了图谱中。在word2vec中，Skipgram的核心思想是<strong>出现在相似上下文中的单词往往具有相似的含义，因此应具有相似的矢量表示</strong>。</p><p>之前讲过的Doc2vec的也实在此基础上提出的。</p><p>根据文章中graph2vec的思想，我们可以把一个图谱看作是一个文件（document），把图谱中的所有节点（node）周围的有根子图（rooted subgraph）看作是词（words）。换句话说，<strong>有根子图构成图谱的方式和词构成句子或段落的方式相同</strong>。具体形式如下：</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200909120140.png" alt="图1 a为doc2vec的skip gram模型，b为graph2vec" style="zoom:50%;" /><h3 id="为什么选用有根子图（rooted-subgraph）"><a href="#为什么选用有根子图（rooted-subgraph）" class="headerlink" title="为什么选用有根子图（rooted subgraph）"></a>为什么选用有根子图（rooted subgraph）</h3><p>节点、步长和路径同样能够组成完整的图谱，那为什么要选择有根子图呢？文章给出了两个理由：</p><ol><li>与节点相比，子图是一种更有序的结构；</li><li>与步长和路径相比，有根子图能够更好的捕获图谱中的非线性特征。因其具有图核的特性。（作者引用了一些实验结果来证明核方法能够更好的捕捉非线性特征）</li></ol><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>首先，graph2vec是一种<strong>无监督</strong>的算法，设计思路参考了doc2vec，因此再熟悉grap2vec之前，我们先复习一下doc2vec。</p><h3 id="doc2vec"><a href="#doc2vec" class="headerlink" title="doc2vec"></a>doc2vec</h3><p>给定一个属于文档$d_i$的词$w_j$，我们要使下列预测结果R尽可能地大：<br>$$<br>R(d_i)=\sum_{j=1}^{l_{i}} \log \operatorname{Pr}\left(w_{j} \mid d_{i}\right)<br>$$<br>式中的可能性$\operatorname{Pr}\left(w_{j} \mid d\right)$，即某个段落中出现$w_j$的可能性，能够定义为:</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200909171931.png"   /><p>我们可以使用负采样的方法有效的训练R。</p><h3 id="graph2vec"><a href="#graph2vec" class="headerlink" title="graph2vec"></a>graph2vec</h3><p>其主要算法流程如下:</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200909145057.png" alt="算法1 graph2vec"></p><h4 id="如何提取有根子图？"><a href="#如何提取有根子图？" class="headerlink" title="如何提取有根子图？"></a>如何提取有根子图？</h4><p>上面算法中的第8行用到了一个提取子图的函数<code>GetWLSubgraph(n,Gi,d)</code>，下面来看看这个函数具体是怎么样的</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200909163921.png" alt="GetWLSubgraph" style="zoom:67%;" /><p>文章中对这个函数给出了比较详细的解释，但是不够直观，所以我画了一个简单的例子。</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200909163601.png" alt="提取有根子图的一个实例" style="zoom:67%;" /><h4 id="如何进行负采样？"><a href="#如何进行负采样？" class="headerlink" title="如何进行负采样？"></a>如何进行负采样？</h4><p>我们很容易能够看出来，算法一中的学习过程是非常昂贵的，因为整个子图词汇表会非常大。因此文章中采用了负采样的方法提高效率。即在训练图$G_i$时，引入不属于$G_i$的子图集$c={sg_1,sg_2,…}$，当然$c \in SG_{vocab}$。</p><h4 id="如何进行优化？"><a href="#如何进行优化？" class="headerlink" title="如何进行优化？"></a>如何进行优化？</h4><p>使用随机梯度下降算法(SGD)来优化算法一9、10两行的参数；</p><p>使用方向传播算法来估算导数；</p><p>学习率$\alpha$按照经验调整。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200909170841.jpeg" alt="一图理解graph2vec结构" style="zoom: 25%;" />]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原论文：graph2vec: Learning Distributed Representations of Graphs&lt;/p&gt;
&lt;p&gt;来源：MLG 2017 - 13th International Workshop on Mining and Learning wit</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="表示学习" scheme="https://cliccker.top/tags/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://cliccker.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="图嵌入" scheme="https://cliccker.top/tags/%E5%9B%BE%E5%B5%8C%E5%85%A5/"/>
    
  </entry>
  
  <entry>
    <title>抓取某网站的摄影作品</title>
    <link href="https://cliccker.top/post/25022.html"/>
    <id>https://cliccker.top/post/25022.html</id>
    <published>2020-08-26T05:12:23.000Z</published>
    <updated>2020-08-26T06:26:57.453Z</updated>
    
    <content type="html"><![CDATA[<p>最近逛知乎找到了一个神奇的网站<a href="https://seaside-station.com/" target="_blank" rel="noopener">SEASIDE STATION IN JAPAN</a></p><p>这个网站记录了大大小小182个靠海车站的景色</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200826131956.png" alt="网站首页"></p><p>其中不乏一些我比较喜欢的图片，比如</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200826132210.png" alt="wabuka站" style="zoom: 50%;" /><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200826132255.png" alt="umishibaura站" style="zoom: 50%;" /><p>许多图片我都想保存下来，但是那样做实在太麻烦了，所以选择用爬虫的方式，代码如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> requests<span class="token keyword">import</span> re<span class="token keyword">import</span> time<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm<span class="token keyword">def</span> <span class="token function">get_url</span><span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">:</span>    wb_date <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">.</span>text    res <span class="token operator">=</span> re<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>r<span class="token string">'***"'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 保护一下这个网站</span>    reg <span class="token operator">=</span> re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span>res<span class="token punctuation">,</span> wb_date<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>reg<span class="token punctuation">)</span>    <span class="token keyword">return</span> reg<span class="token keyword">def</span> <span class="token function">get_picture</span><span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">:</span>    wb_date <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>text    res <span class="token operator">=</span> re<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>r<span class="token string">'src="(http.+?jpg)"'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 正则表达式匹配图片</span>    reg <span class="token operator">=</span> re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span>res<span class="token punctuation">,</span> wb_date<span class="token punctuation">)</span>    nun <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>reg<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 遍历</span>        a <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>i<span class="token punctuation">)</span>        name <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">"***"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span>        f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'all_station/'</span> <span class="token operator">+</span> name<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>a<span class="token punctuation">.</span>content<span class="token punctuation">)</span>        f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        nun <span class="token operator">=</span> nun <span class="token operator">+</span> <span class="token number">1</span>urls <span class="token operator">=</span> get_url<span class="token punctuation">(</span><span class="token string">'***'</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> urls <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>urls<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        get_picture<span class="token punctuation">(</span>urls<span class="token punctuation">)</span>    <span class="token keyword">except</span><span class="token punctuation">:</span>        <span class="token keyword">continue</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span></code></pre><p>如果你也喜欢这些图片，可以在评论里索取，千万不要学我！！！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近逛知乎找到了一个神奇的网站&lt;a href=&quot;https://seaside-station.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SEASIDE STATION IN JAPAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个网站记录了大大小小182个靠</summary>
      
    
    
    
    <category term="实践" scheme="https://cliccker.top/categories/%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="python" scheme="https://cliccker.top/tags/python/"/>
    
    <category term="爬虫" scheme="https://cliccker.top/tags/%E7%88%AC%E8%99%AB/"/>
    
    <category term="摄影" scheme="https://cliccker.top/tags/%E6%91%84%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow读书笔记</title>
    <link href="https://cliccker.top/post/53334.html"/>
    <id>https://cliccker.top/post/53334.html</id>
    <published>2020-08-24T10:52:19.000Z</published>
    <updated>2020-08-25T07:53:42.403Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hello-World！"><a href="#Hello-World！" class="headerlink" title="Hello World！"></a>Hello World！</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tfh <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token string">"Ciao!"</span><span class="token punctuation">)</span>w <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token string">"World"</span><span class="token punctuation">)</span>hw <span class="token operator">=</span> h <span class="token operator">+</span> w<span class="token keyword">with</span> tf<span class="token punctuation">.</span>compat<span class="token punctuation">.</span>v1<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    ans <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>hw<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>ans<span class="token punctuation">)</span><span class="token operator">></span>b<span class="token string">'Ciao!World'</span></code></pre><p>这一段代码展示了<strong>计算图</strong>的主要思想，即首先定义计算需要的元素，再采用一个外部机制去触发这个计算。也就是说<code>hw = h + w</code>并<strong>没有</strong>执行操作，真正执行操作的是<code>ans = sess.run(hw)</code></p><h2 id="SoftmaxMnist"><a href="#SoftmaxMnist" class="headerlink" title="SoftmaxMnist"></a>SoftmaxMnist</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>examples<span class="token punctuation">.</span>tutorials<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> input_datadata_dir <span class="token operator">=</span> <span class="token string">'data'</span>num_steps <span class="token operator">=</span> <span class="token number">1000</span>minibatch_size <span class="token operator">=</span> <span class="token number">100</span>data <span class="token operator">=</span> input_data<span class="token punctuation">.</span>read_data_sets<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> one_hot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 自动加载数据集</span>x <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># None表示当前不指定每次使用图片的数量</span>W <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 权重</span>y_true <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>y_pred <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">)</span>cross_entropy <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax_cross_entropy_with_logits<span class="token punctuation">(</span>logits<span class="token operator">=</span>y_pred<span class="token punctuation">,</span> labels<span class="token operator">=</span>y_true<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用交叉熵作为loss</span>gd_step <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>GradientDescentOptimizer<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>cross_entropy<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 定义如何训练</span>correct_mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>correct_mask<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch_xs<span class="token punctuation">,</span> batch_ys <span class="token operator">=</span> data<span class="token punctuation">.</span>train<span class="token punctuation">.</span>next_batch<span class="token punctuation">(</span>minibatch_size<span class="token punctuation">)</span>        sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>gd_step<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>x<span class="token punctuation">:</span> batch_xs<span class="token punctuation">,</span> y_true<span class="token punctuation">:</span> batch_ys<span class="token punctuation">}</span><span class="token punctuation">)</span>    ans <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>accuracy<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>x<span class="token punctuation">:</span> data<span class="token punctuation">.</span>test<span class="token punctuation">.</span>images<span class="token punctuation">,</span> y_true<span class="token punctuation">:</span> data<span class="token punctuation">.</span>test<span class="token punctuation">.</span>labels<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Accuracy: {:.4}%"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>ans<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>由于这本书是两年前的，很多模块都有了变化，后面我找了另一本更新的TensorFlow教程来看。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Hello-World！&quot;&gt;&lt;a href=&quot;#Hello-World！&quot; class=&quot;headerlink&quot; title=&quot;Hello World！&quot;&gt;&lt;/a&gt;Hello World！&lt;/h2&gt;&lt;pre class=&quot; language-python&quot;&gt;&lt;co</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="python" scheme="https://cliccker.top/tags/python/"/>
    
    <category term="TensorFlow" scheme="https://cliccker.top/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>gensim中的Doc2vec模型参数调整</title>
    <link href="https://cliccker.top/post/85dd.html"/>
    <id>https://cliccker.top/post/85dd.html</id>
    <published>2020-08-16T07:28:00.000Z</published>
    <updated>2020-08-17T04:10:11.835Z</updated>
    
    <content type="html"><![CDATA[<p>最近在用Doc2vec训练模型，现在现在需要做一些参数调整来提高准确率。这边记录一些参数的调整和效果。</p><pre class=" language-python"><code class="language-python">Doc2Vec<span class="token punctuation">(</span>documents<span class="token operator">=</span>None<span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">#输入语料库</span>        corpus_file<span class="token operator">=</span>None<span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">#LineSentence格式的语料库文件的路径。</span>        alpha <span class="token operator">=</span> float<span class="token punctuation">,</span>        <span class="token comment" spellcheck="true">#初始学习率</span>        seed <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">#随机种子</span>        negative <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span>        <span class="token comment" spellcheck="true">#如果数值大于零，则加入负面采样。数值多大就加入多少个“noise word” </span>        dm_mean<span class="token operator">=</span>None<span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">#当使用DM训练算法时，对上下文向量相加（默认0）；若设为1，则求均值</span>        dm<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>          <span class="token comment" spellcheck="true">#默认值为1，表示使用DM模型，否则使用DBOW模型</span>        dbow_words<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">#当设为1时，则在训练doc_vector（DBOW）的同时训练Word_vector（Skip-gram）；默认为0，          只训练doc_vector，速度更快。</span>        dm_concat<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">#默认为0，当设为1时，在使用DM训练算法时，直接将上下文向量和Doc向量拼接。</span>        dm_tag_count<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">#使用dm_concat模式时，每个文档的预期文档标签数。</span>        sammple <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span>        <span class="token comment" spellcheck="true">#用于配置随机采样哪些高频词的阈值，有用范围是（0，1e-5）。</span>        min_count <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span>        <span class="token comment" spellcheck="true">#忽略所有词频少于一定数值的单词</span>        docvecs<span class="token operator">=</span>None<span class="token punctuation">,</span>         docvecs_mapfile<span class="token operator">=</span>None<span class="token punctuation">,</span>         comment<span class="token operator">=</span>None<span class="token punctuation">,</span>         trim_rule<span class="token operator">=</span>None<span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">#词汇修剪原则，指定某些单词应保留在词汇中</span>        callbacks<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">#回调列表</span>        <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在用Doc2vec训练模型，现在现在需要做一些参数调整来提高准确率。这边记录一些参数的调整和效果。&lt;/p&gt;
&lt;pre class=&quot; language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;Doc2Vec&lt;span class=&quot;t</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="gensim" scheme="https://cliccker.top/tags/gensim/"/>
    
    <category term="Doc2vec" scheme="https://cliccker.top/tags/Doc2vec/"/>
    
    <category term="python" scheme="https://cliccker.top/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>读论文——Sentence-BERT:基于孪生网络的句子嵌入</title>
    <link href="https://cliccker.top/post/947f.html"/>
    <id>https://cliccker.top/post/947f.html</id>
    <published>2020-08-11T04:47:00.000Z</published>
    <updated>2020-11-09T04:24:56.019Z</updated>
    
    <content type="html"><![CDATA[<p>原标题：Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</p><p><a href="https://arxiv.org/abs/1908.10084" target="_blank" rel="noopener">论文地址</a></p><p><a href="https://github.com/UKPLab/sentence-transformers" target="_blank" rel="noopener">代码</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><code>BERT</code>（2018）和<code>RoBERTa</code>（2019）在处理文本语义相似度等句子对回归的任务上，都有不错的效果。但BERT在计算句子相似度时，需要将两个句子都输入到网络中，这就会产生巨大的计算开销。在10000个句子中找到相似的句子需要<code>5000w</code>次推理计算，这项任务在V100GPU上大约耗时<code>65h</code>。</p><p><strong>BERT的构造使得它既不适用于语义相似度搜索，也不适合于聚类等无监督任务。</strong></p><p>有研究尝试将整个句子输入到BERT中，得到该句的句向量。但是这样得到的句向量包含的语义信息有限，也就是说</p><p><strong>相似的句子的句向量差别可能会很大</strong></p><p>这一点作者也在后续的实验中证明了：</p><blockquote><p>The results shows that directly using the output of BERT leads to rather poor performances. Averaging the BERT embeddings achieves an average correlation of only 54.81, and using the CLStoken output only achieves an average correlationof 29.19. Both are worse than computing average GloVe embeddings.</p></blockquote><h2 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h2><p>提出了Sentence-Bert，一种改进的模型。该模型使用孪生网络（Siamese Network）来输出<strong>定长</strong>的、<strong>保有语义信息</strong>的句向量。再通过余弦距离、曼哈顿距离或欧式距离来计算其相似度。SBERT可以将原本需要<code>65h</code>的任务缩短至<code>5s</code>左右。</p><h2 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h2><h3 id="孪生网络（Siamse-Network）"><a href="#孪生网络（Siamse-Network）" class="headerlink" title="孪生网络（Siamse Network）"></a>孪生网络（Siamse Network）</h3><img src="C:\Users\76084\Desktop\20200811192235.png" alt="孪生网络" style="zoom: 33%;" /><p>孪生网络结构相对简单，在计算句子相似度方面有很多良好的表现。最主要的特征是孪生网络的两个解码器（encoder）共享权重<code>W</code>。在输出层，可以通过余弦相似度等方法比较$u,v$两个向量之间的相似度。也可以用额外的模型来生成两个句子之间<strong>关系的特征向量</strong>，用于问答系统等更复杂的任务。</p><h3 id="使用BERT处理文本匹配任务"><a href="#使用BERT处理文本匹配任务" class="headerlink" title="使用BERT处理文本匹配任务"></a>使用BERT处理文本匹配任务</h3><img src="C:\Users\76084\Desktop\20200811200024.png" alt="BERT处理文本匹配任务" style="zoom: 50%;" /><p>如图，<code>BERT</code>的常规做法是将两个句子拼接成一个序列，中间以<code>SEP</code>符号进行分隔，经过特定模块进行编码后，取输出层的字向量的<code>平均值</code>或第一个特征位置<code>CLS</code>作为句子的句向量。这种取平均值或特征值的做法使得<code>BERT</code>输出的句向量不包含充分的语义信息。</p><h3 id="Sentence-Bert"><a href="#Sentence-Bert" class="headerlink" title="Sentence-Bert"></a>Sentence-Bert</h3><p>SBERT沿用了孪生网络的结构，两个Sentence Encoder使用的是同一个BERT，并在其后加入了一个池化（pooling）操作来实现输出相同大小的句向量。文章一共试验了三种池化策略：</p><ol><li>CLS-token 以特征位置向量作为句向量</li><li>MEAN-strategy 求取所有输出向量的平均值（默认）</li><li>MAX-strategy 求取所有输出向量中的最大值</li></ol><p>通过池化操作进一步将BERT输出后的字向量进行特征提取、压缩，得到$u,v$。</p><p>最后将$u,v$进行组合，针对不同的任务提出不同的组合方式</p><h4 id="针对分类任务"><a href="#针对分类任务" class="headerlink" title="针对分类任务"></a>针对<strong>分类任务</strong></h4><img src="C:\Users\76084\Desktop\20200811200024.png" alt="SBERT的一种训练模型" style="zoom:50%;" /><p>针对分类任务的训练模型如图1所示。使用元素级差分$|u-v|$来组合$u,v$，将其乘以一个可训练的权重矩阵$W_{t} \in \mathbb{R}^{3 n \times k}$（n为句向量的维度，k为标签的个数）。使用<code>Softmax</code>函数进行分类输出</p><p>$$<br>o=\operatorname{softmax}\left(W_{t}(u, v,|u-v|)\right)<br>$$</p><p>其中矩阵的形式如下：</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200813084959.png" alt="矩阵形式" style="zoom:67%;" /><p>最后使用交叉熵来作为优化，文章中没有提到具体形式，查到其大致形式如下：</p><p>$$<br>L=\frac{1}{N} \sum_{i}^{N} L_{i}=\frac{1}{N} \sum_{i}^{N}\sum_{c=1}^{M} y_{i c} \log \left(p_{i c}\right)<br>$$<br>其中：</p><p>$M$—— 类别的数量；</p><p>$N$—— 样本的数量；</p><p>$y_c$—— 指示变量（0或1)，如果该类别和样本的类别相同就是1，否则是0；</p><p>$p_c$——对于观测样本属于类别$c$的预测概率。</p><h4 id="针对相似度计算"><a href="#针对相似度计算" class="headerlink" title="针对相似度计算"></a>针对<strong>相似度</strong>计算</h4><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200812131036.png" alt="相似度计算" style="zoom: 50%;" /><p>直接计算、输出余弦相似度；训练损失函数采取了均方根误差；</p><p>$$<br>\sigma=\sqrt{\frac{\sum d_{i}^{2}}{n}}, \mathrm{i}=1<br>$$</p><h4 id="针对三元组输入"><a href="#针对三元组输入" class="headerlink" title="针对三元组输入"></a>针对<strong>三元组输入</strong></h4><img src="C:\Users\76084\Desktop\20200812184837.png" alt="针对三元组的模型" style="zoom: 33%;" /><p>给定一个锚定句$a$，正面句$p$，负面句$n$。这个网络的目标是调整三元组中$a$与$p$的距离小于$a$与$n$的距离。其损失函数如下：</p><p>$$<br>\max \left(\left|s_{a}-s_{p}\right|-\left|s_{a}-s_{n}\right|+\epsilon, 0\right)<br>$$</p><p>$s_x$—— $a/n/p$的句向量；</p><p>$||·||$—— 计算距离，如欧式距离、曼哈顿距离；</p><p>$\epsilon$ —— 边界，保证$s_p$与$s_a$之间的距离至少要比$s_n$与$s_a$小$\epsilon$，文中取1</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="比较组合方式和池化策略的差异"><a href="#比较组合方式和池化策略的差异" class="headerlink" title="比较组合方式和池化策略的差异"></a>比较组合方式和池化策略的差异</h3><img src="C:\Users\76084\Desktop\20200812134015.png" alt="不同池化策略和组合方式的差异" style="zoom:67%;" /><p>可以看到平均池化策略的表现比较好；</p><p>再平均池化策略下，使用$(u, v,|u-v|)$组合方式表现比较好。</p><h3 id="计算效率"><a href="#计算效率" class="headerlink" title="计算效率"></a>计算效率</h3><img src="C:\Users\76084\Desktop\20200812134540.png" alt="不同嵌入模型的计算效率比较" style="zoom:67%;" /><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文提出了在Siamse Network上改进而来的Sentence-BERT模型，通过其实验结果可以看出，该模型在计算效率上有非常大的提高，可能会在工业领域有比较好的应用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原标题：Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.10084&quot; target=&quot;_blank&quot; rel=</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="问答系统" scheme="https://cliccker.top/tags/%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="NLP" scheme="https://cliccker.top/tags/NLP/"/>
    
    <category term="Bert" scheme="https://cliccker.top/tags/Bert/"/>
    
  </entry>
  
  <entry>
    <title>结构化标准的教程</title>
    <link href="https://cliccker.top/post/85dd.html"/>
    <id>https://cliccker.top/post/85dd.html</id>
    <published>2020-08-03T07:06:00.000Z</published>
    <updated>2020-08-09T10:19:50.574Z</updated>
    
    <content type="html"><![CDATA[<h2 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h2><ul><li>福昕PDF阅读器</li><li>office2016以上版本或wps</li><li>Typora Markdown</li></ul><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol><li><p>使用word打开对应PDF文档，等待文档转换成可以编辑的格式。</p></li><li><p>打开识别成功的文件，清除所有的格式，将所用空格替换掉，全部<code>．</code>和<code>·</code>替换成<code>.</code>，全部<code>一</code>替换成<code>-</code></p></li><li><p>标题的处理</p><p>Markdown中的语法，#+“空格”+“文字” 就能设置标题。也可以在标题行按ctrl+“级数”设置标题等级。具体格式参考</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200803165011.png" alt="标题示例"></p></li><li><p>段落的处理</p><p>检查是否有错误和纰漏，确认无误之后将对应标题下的文字复制粘贴到Typora里。</p><p>粘贴之后用代码模式检查是否存在图片，如果存在就删除。</p></li><li><p>公式、表和图的处理</p><p>段落中引用公式参考</p><blockquote><p>计入强度计算的复合材料的折算厚度可按式（3-1a）计算:</p></blockquote><p>公式本身用￥+“公式编号”+￥表示，单独占一行，示例</p><blockquote><p>￥式（3-1a）￥</p></blockquote><p>句中的公式和元素用￥￥代替，示例</p><blockquote><p>本标准所用材料的设计应力强度值￥￥</p></blockquote><p>段落中引用表格，示例</p><blockquote><p>在设计容器各部分时，必须按表3-1考虑压力、温度及静压头之间的相互关系。</p></blockquote><p>表格用如下格式表示</p><blockquote><p>￥表3-1 ￥压力与温度之间的关系</p></blockquote><p>图片的处理方式和表格相同</p></li><li><p>在处理名词术语时，要尽可能地分级，例如</p></li></ol><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200805142052.png" alt="示例"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;软件&quot;&gt;&lt;a href=&quot;#软件&quot; class=&quot;headerlink&quot; title=&quot;软件&quot;&gt;&lt;/a&gt;软件&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;福昕PDF阅读器&lt;/li&gt;
&lt;li&gt;office2016以上版本或wps&lt;/li&gt;
&lt;li&gt;Typora Markdown&lt;/l</summary>
      
    
    
    
    <category term="技巧" scheme="https://cliccker.top/categories/%E6%8A%80%E5%B7%A7/"/>
    
    
    <category term="PDF" scheme="https://cliccker.top/tags/PDF/"/>
    
    <category term="word" scheme="https://cliccker.top/tags/word/"/>
    
    <category term="结构化" scheme="https://cliccker.top/tags/%E7%BB%93%E6%9E%84%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>人们怕人工智能获得意识到底是怕哪个方面？</title>
    <link href="https://cliccker.top/post/4014.html"/>
    <id>https://cliccker.top/post/4014.html</id>
    <published>2020-07-29T04:46:54.000Z</published>
    <updated>2020-08-09T10:19:50.564Z</updated>
    
    <content type="html"><![CDATA[<p>我一直认为人类的许多表现，都是原始本能在现代社会中的映射。编造个小故事：</p><p>在这片原始的大陆上，生活着许多不同的原始部族。<code>咚塔塔人</code>也是其中一员，或许是因为脑容量大一丢丢吧，咚塔塔人首先悟出了三条生存的准则：</p><ol><li>猎物的肉-火烤-吃；</li><li>猎物的皮毛-剥下来-穿在身上-不冷 ；</li><li>猎物的腿骨-敲碎-敌人的脑袋。</li></ol><p>很快他们悟到了第四条准则：</p><ol start="4"><li>笨蛋脑袋可以不敲碎，拿来生孩子当苦力</li></ol><p>接着是第五条：</p><ol start="5"><li>笨蛋脑袋不好用，教会前三条就好用了</li></ol><p>然后是笨蛋脑袋中学会了前三条，并悟出了第六条：</p><ol start="6"><li>咚塔塔人能做到的，我们也能！敲碎他们的脑袋！</li></ol><p>我想表达的是，令人们感到恐惧的不是人工智能产生意识——我们可以利用这些意识实现更多的价值——而是人工智能<strong>越来越接近</strong>人类。从某种意义上来说，人们就是在害怕自己。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我一直认为人类的许多表现，都是原始本能在现代社会中的映射。编造个小故事：&lt;/p&gt;
&lt;p&gt;在这片原始的大陆上，生活着许多不同的原始部族。&lt;code&gt;咚塔塔人&lt;/code&gt;也是其中一员，或许是因为脑容量大一丢丢吧，咚塔塔人首先悟出了三条生存的准则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;猎</summary>
      
    
    
    
    <category term="想法" scheme="https://cliccker.top/categories/%E6%83%B3%E6%B3%95/"/>
    
    
    <category term="科幻" scheme="https://cliccker.top/tags/%E7%A7%91%E5%B9%BB/"/>
    
  </entry>
  
  <entry>
    <title>向量相似度计算方法</title>
    <link href="https://cliccker.top/post/1d51.html"/>
    <id>https://cliccker.top/post/1d51.html</id>
    <published>2020-07-27T06:31:00.000Z</published>
    <updated>2020-08-09T10:19:50.566Z</updated>
    
    <content type="html"><![CDATA[<p>最近在做的嵌入模型比较，需要用到比较向量相似度，在知乎上看到了<a href="https://zhuanlan.zhihu.com/p/33164335" target="_blank" rel="noopener">一篇文章</a>，简单搬运过来做一些笔记和代码实践。首先列举一些向量相似度计算的方法：</p><ol><li>欧式距离（Euclidean Distance）</li><li>余弦相似度（Cosine Similarity）</li><li>皮尔逊相关系数（Pearson）</li><li>修正余弦相似度（Adjusted Cosine）</li><li>汉明距离（Hamming Distance）</li><li>曼哈顿距离（Manhattan Distance）</li><li>切比雪夫距离（Chebyshev Distance）</li></ol><h2 id="欧式距离（Euclidean-Distance）"><a href="#欧式距离（Euclidean-Distance）" class="headerlink" title="欧式距离（Euclidean Distance）"></a>欧式距离（Euclidean Distance）</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>欧氏距离比较容易理解，就是两点之间的直线距离，二以此类推空间中的两点$a(x_1,y_1)和b(x_2,y_2)$的欧式距离可以表示为：<br>$$<br>d=\sqrt{(x_1-x_2)^{2}+(y_1-y_2)^{2}}<br>$$<br>多维空间中的两点之间欧式距离可以表示为：<br>$$<br>d=\sqrt{(x_1-x_2)^{2}+(y_1-y_2)^{2}+(z_1-z_2)^{2}+···}<br>$$</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>python简单实现：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">EuclideanDistance</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    d <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> zip<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># zip将两个列表打包为元组的列表</span>        d <span class="token operator">+=</span> <span class="token punctuation">(</span>a <span class="token operator">-</span> b<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>    <span class="token keyword">return</span> d <span class="token operator">**</span> <span class="token number">0.5</span></code></pre><p>使用numpy计算：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">EuclideanDistance_np</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># np.linalg.norm 用于范数计算，默认是二范数，相当于平方和开根号</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>经测试，对于同样的两组向量，两个函数的结果相同。</p><h2 id="余弦相似度（Cosine-Similarity）"><a href="#余弦相似度（Cosine-Similarity）" class="headerlink" title="余弦相似度（Cosine Similarity）"></a>余弦相似度（Cosine Similarity）</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><blockquote><p>首先，样本数据的夹角余弦并不是真正几何意义上的夹角余弦，只不过是借了它的名字，实际是借用了它的概念变成了是代数意义上的“夹角余弦”，用来衡量样本向量间的差异。</p></blockquote><p>夹角越小，余弦值越接近1，反之越接近-1。假设有两个向量$\vec x_1,\vec x_2$:<br>$$<br>\cos (\theta)=\frac{\sum_{k=1}^{n} x_{1 k} x_{2 k}}{\sqrt{\sum_{k=1}^{n} x_{1 k}^{2}} \sqrt{\sum_{k=1}^{n} x_{2 k}^{2}}}<br>$$</p><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><p>用python实现该公式：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Cosine</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    sum_xy <span class="token operator">=</span> <span class="token number">0</span>    num_x <span class="token operator">=</span> <span class="token number">0</span>    num_y <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> zip<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        sum_xy <span class="token operator">+=</span> a <span class="token operator">*</span> b        num_x <span class="token operator">+=</span> a <span class="token operator">**</span> <span class="token number">2</span>        num_y <span class="token operator">+=</span> b <span class="token operator">**</span> <span class="token number">2</span>    <span class="token keyword">if</span> num_x <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">or</span> num_y <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 判断分母是否为零</span>        <span class="token keyword">return</span> None    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> sum_xy <span class="token operator">/</span> <span class="token punctuation">(</span>num_y <span class="token operator">*</span> num_x<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span>V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>V_z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 0.9956602816447043</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 1.0</span></code></pre><p>用numpy简化计算过程，用相同的向量测试：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Cosine_np</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span>    d <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>b<span class="token punctuation">)</span>     <span class="token keyword">return</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span> <span class="token operator">/</span> d V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>V_z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.9956602816447043</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1.0000000000000002</span></code></pre><h2 id="欧式距离和余弦相似度的差异"><a href="#欧式距离和余弦相似度的差异" class="headerlink" title="欧式距离和余弦相似度的差异"></a>欧式距离和余弦相似度的差异</h2><p>来看输出结果的对比</p><pre class=" language-python"><code class="language-python">V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>V_z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.9956602816447043</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1.0000000000000002</span><span class="token keyword">print</span><span class="token punctuation">(</span>EuclideanDistance_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1.7320508075688772</span><span class="token keyword">print</span><span class="token punctuation">(</span>EuclideanDistance_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 15.362291495737216</span></code></pre><p>从中可以看出</p><ul><li>$\vec x,\vec z$同向，他们的余弦距离为1，说明这两个向量<strong>方向一致</strong>，但是两者的欧式距离相差甚远，说明两者<strong>数值相差</strong>较大。</li><li>余弦相似度用来衡量两个向量之间的<strong>变化趋势</strong>，而欧式距离可以比较两个向量的<strong>数值差异</strong></li></ul><h2 id="皮尔逊相关系数（Pearson-Correlation-Coefficient）"><a href="#皮尔逊相关系数（Pearson-Correlation-Coefficient）" class="headerlink" title="皮尔逊相关系数（Pearson Correlation Coefficient）"></a>皮尔逊相关系数（Pearson Correlation Coefficient）</h2><h3 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h3><p>其公式如下：<br>$$<br>\operatorname{sim}\left(x_{1}, x_{2}\right)=\frac{\sum_{k=1}^{n}\left(x_{1 k}-\overline{x_{1}}\right)\left(x_{2 k}-\overline{x_{2}}\right)}{\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-\overline{x_{1}}\right)^{2}} \sqrt{\sum_{k=1}^{n}\left(x_{2 k}-\overline{x_{2}}\right)^{2}}}<br>$$<br>$\overline x$表示均值</p><p>余弦相似度会受到向量的平移影响，为了实现平移不变性，在余弦相似度的基础上，每个向量减去这个向量均值组成的向量，也就是皮尔逊相关系数。</p><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><p>python</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Pearson</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token punctuation">:</span>    sum_xy <span class="token operator">=</span> <span class="token number">0</span>    num_x <span class="token operator">=</span> <span class="token number">0</span>    num_y <span class="token operator">=</span> <span class="token number">0</span>    avr_x <span class="token operator">=</span> sum<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">/</span> len<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 求平均值</span>    avr_y <span class="token operator">=</span> sum<span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token operator">/</span> len<span class="token punctuation">(</span>y<span class="token punctuation">)</span>    <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> zip<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        sum_xy <span class="token operator">+=</span> <span class="token punctuation">(</span>a<span class="token operator">-</span>avr_x<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>b<span class="token operator">-</span>avr_y<span class="token punctuation">)</span>        num_x <span class="token operator">+=</span> <span class="token punctuation">(</span>a<span class="token operator">-</span>avr_x<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>        num_y <span class="token operator">+=</span> <span class="token punctuation">(</span>b<span class="token operator">-</span>avr_y<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>    <span class="token keyword">if</span> num_x <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">or</span> num_y <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 判断分母是否为零</span>        <span class="token keyword">return</span> None    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> sum_xy <span class="token operator">/</span> <span class="token punctuation">(</span>num_y <span class="token operator">*</span> num_x<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span><span class="token comment" spellcheck="true">#0.9831290611762872</span><span class="token comment" spellcheck="true">#1.0</span></code></pre><p>引入numpy：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Pearson_np</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># .corrcoef()是numpy中内置的计算皮尔逊相关系数的方法，同时需要进行归一化处理</span>    <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>corrcoef<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> rowvar<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200727192841.png" alt="关于.corrcoef()返回的值" style="zoom:67%;" /><p>当然也可以不使用该方法计算相似度，这里不多解释。</p><h2 id="余弦相似度于皮尔逊相关系数的比较"><a href="#余弦相似度于皮尔逊相关系数的比较" class="headerlink" title="余弦相似度于皮尔逊相关系数的比较"></a>余弦相似度于皮尔逊相关系数的比较</h2><pre class=" language-python"><code class="language-python">V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>V_z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1.0</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.9450766454656805</span><span class="token keyword">print</span><span class="token punctuation">(</span>Pearson_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1.0</span><span class="token keyword">print</span><span class="token punctuation">(</span>Pearson_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.819092959946616</span></code></pre><p>从这个结果不容易理解皮尔逊相似系数。关于如何解释Pearson，我觉得<a href="https://www.zhihu.com/question/19734616/answer/174098489" target="_blank" rel="noopener">这个回答</a>写的比较好</p><p>他解释了为什么要<strong>中心化</strong></p><blockquote><p>中心化的意思是说, 对每个向量, 我先计算所有元素的平均值avg, 然后向量中每个维度的值都减去这个avg, 得到的这个向量叫做被中心化的向量. 机器学习, 数据挖掘要计算向量余弦相似度的时候, 由于向量经常在某个维度上有数据的缺失, 预处理阶段都要对所有维度的数值进行中心化处理.</p></blockquote><h2 id="修正余弦相似度（Adjusted-Cosine-Similarity）"><a href="#修正余弦相似度（Adjusted-Cosine-Similarity）" class="headerlink" title="修正余弦相似度（Adjusted Cosine Similarity）"></a>修正余弦相似度（Adjusted Cosine Similarity）</h2><h3 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h3><p>正如前文所说，余弦相似度对数值并不敏感，这种不敏感会使数值出现误差，因此我们要对其进行修正。</p><p>🌰：假设A用户为两部电影打分（1，2）B用户打分（9，10），这两个分数的余弦相似度是0.96，但是很显然，A并没有B那么喜欢第二部电影，这就产生了误差。</p><p>如何避免这种误差？答案是再引入去中心化的方法。其公式可以写成：<br>$$<br>\operatorname{adjcos<br>}\left(x_{1}, x_{2}\right)=\frac{\sum_{k=1}^{n}\left(x_{1 k}-\overline{x_{11}+x_{21}}\right)\left(x_{2 k}-\overline{x_{21}+x_{11}}\right)}{\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-\overline{x_{11}+x_{21}}\right)^{2}} \sqrt{\sum_{k=1}^{n}\left(x_{2 k}-\overline{x_{21}+x_{11}}\right)^{2}}}<br>$$<br>仔细观察，他和公式（4）差别在哪里？每一项都减去了向量中第一项的平均值。</p><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><p>尝试用python实现：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">AdjCosine</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    sum_xy <span class="token operator">=</span> <span class="token number">0</span>    num_x <span class="token operator">=</span> <span class="token number">0</span>    num_y <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> zip<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        avr <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>        sum_xy <span class="token operator">+=</span> <span class="token punctuation">(</span>a <span class="token operator">-</span> avr<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>b <span class="token operator">-</span> avr<span class="token punctuation">)</span>        num_x <span class="token operator">+=</span> <span class="token punctuation">(</span>a <span class="token operator">-</span> avr<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>        num_y <span class="token operator">+=</span> <span class="token punctuation">(</span>b <span class="token operator">-</span> avr<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>    <span class="token keyword">if</span> num_x <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">or</span> num_y <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 判断分母是否为零</span>        <span class="token keyword">return</span> None    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>sum_xy <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>num_y <span class="token operator">*</span> num_x<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>使用numpy简化：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">AdjCosine_np</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span>    avr <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>    d <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>a<span class="token operator">-</span>avr<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>b<span class="token operator">-</span>avr<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token operator">-</span>avr<span class="token punctuation">,</span> b<span class="token operator">-</span>avr<span class="token punctuation">)</span> <span class="token operator">/</span> d<span class="token punctuation">)</span></code></pre><p>于余弦相似度进行对比：</p><pre class=" language-python"><code class="language-python">V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span>V_z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>AdjCosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.951797128930044</span><span class="token keyword">print</span><span class="token punctuation">(</span>AdjCosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.6889822365046137</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1.0</span><span class="token keyword">print</span><span class="token punctuation">(</span>Cosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.9913538149119954</span></code></pre><p>可以看到两者的差别还是挺大的，说明数值的确产生了比较大的影响。</p><p>❓：思考一下，如果向量中的第0个元素相同，要怎么办呢？尝试一下：</p><pre class=" language-python"><code class="language-python">V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>AdjCosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.9985272427507907</span></code></pre><p>果然，结果显示两个向量非常相似。要解决这个问题，我们可以参考Pearson的处理方法，用平均数构造修正函数。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">AdjCosine_np_2</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span>    avr <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 合并矩阵并求矩阵的平均值</span>    d <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>a <span class="token operator">-</span> avr<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>b <span class="token operator">-</span> avr<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a <span class="token operator">-</span> avr<span class="token punctuation">,</span> b <span class="token operator">-</span> avr<span class="token punctuation">)</span> <span class="token operator">/</span> d<span class="token punctuation">)</span>V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span>V_z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>AdjCosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.9985272427507907</span><span class="token keyword">print</span><span class="token punctuation">(</span>AdjCosine_np_2<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.6879115070007071</span><span class="token keyword">print</span><span class="token punctuation">(</span>AdjCosine_np<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.951797128930044</span><span class="token keyword">print</span><span class="token punctuation">(</span>AdjCosine_np_2<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 0.5674199862463242</span></code></pre><p>由结果可见，数值引起的差别被放大了，但同时也保留了余弦相似度的反映变化趋势的特征。</p><h2 id="汉明距离（Hamming-distance）"><a href="#汉明距离（Hamming-distance）" class="headerlink" title="汉明距离（Hamming distance）"></a>汉明距离（Hamming distance）</h2><p>最好理解的一个：字符串之间<strong>对应位不同</strong>的数量，比如“110”和“111”的汉明距离为1，可以用在信号处理上，如果是在向量对比上效率就显得有点低了。</p><h2 id="曼哈顿距离（Manhattan-Distance）"><a href="#曼哈顿距离（Manhattan-Distance）" class="headerlink" title="曼哈顿距离（Manhattan Distance）"></a>曼哈顿距离（Manhattan Distance）</h2><h3 id="定义-4"><a href="#定义-4" class="headerlink" title="定义"></a>定义</h3><p>原文作者在这里提到了<code>刘昊然</code>原来他在唐探里提到过<code>曼哈顿计量法</code>。</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200728145702.jpg" alt="唐探里和作法一样的曼哈顿计量法" style="zoom:67%;" /><p>那曼哈顿距离又是什么呢，可以看这张图：</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200728145953.jpg" alt="曼哈顿距离"></p><p>想象一下你是一个出租车司机，在曼哈顿街头，如果你想从A到B点，理论上最短距离应是直线距离，而实际上你不可能穿过一栋栋房屋直接到达B。曼哈顿距离表示的是你实际驾驶出租车从A到B的距离，该距离等于两个点在标准坐标系上的<strong>绝对轴距</strong>总和。用公式表示即<br>$$<br>\mathrm{d}<em>{12}=\sum</em>{k=1}^{n}\left|\mathrm{x}<em>{1 k}-x</em>{2 k}\right|<br>$$</p><h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Manhattan</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    d <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> zip<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        d <span class="token operator">+=</span> a <span class="token operator">-</span> b    <span class="token keyword">return</span> abs<span class="token punctuation">(</span>d<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 取绝对值</span>V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span>V_z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>Manhattan<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 5</span><span class="token keyword">print</span><span class="token punctuation">(</span>Manhattan<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 6</span></code></pre><h2 id="切比雪夫距离（Chebyshev-Distance）"><a href="#切比雪夫距离（Chebyshev-Distance）" class="headerlink" title="切比雪夫距离（Chebyshev Distance）"></a>切比雪夫距离（Chebyshev Distance）</h2><h3 id="定义-5"><a href="#定义-5" class="headerlink" title="定义"></a>定义</h3><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200728152528.png" alt="国际象棋棋盘上的切比雪夫距离" style="zoom:80%;" /><p>我们可以通过观察这个国际象棋棋盘来理解切比雪夫距离，国王走到棋盘上任意一点的步数，只和坐标差值中较大者有关。</p><p>更科学地定义为</p><blockquote><p>切比雪夫距离：设平面空间内存在两点，它们的坐标为$(x_1,y_1)，(x_2,y_2)$ 则$is=max(|x_1−x_2|,|y_1−y_2|) $。即两点横纵坐标差的最大值 。$dis=max(AC,BC)=AC=4$。两个n维向量$(x_{11},x_{12},…,x_{1n})$与 $b(x_{21},x_{22},…,x_{2n})$间的切比雪夫距离：$d_{a b}=\max \left(\left|x_{1 i}-x_{2 i}\right|\right)$</p></blockquote><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200728153839.png" alt="AC为两点的切比雪夫距离"></p><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Chebyshev</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    d <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y<span class="token punctuation">)</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>d<span class="token punctuation">,</span> <span class="token operator">-</span>d<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># np.maximum(a, -a)这一步相当于在取绝对值</span>V_x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>V_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span>V_z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>Chebyshev<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_y<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 3</span><span class="token keyword">print</span><span class="token punctuation">(</span>Chebyshev<span class="token punctuation">(</span>V_x<span class="token punctuation">,</span> V_z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 10</span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>当然还有别的诸多距离，如闵可夫斯基距离，标准欧式距离。但是考虑到后续工作可能主要放在向量相似度的比较上，考虑使用余弦相似度相关的计算公式更合理。</p><p><strong>以上。</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在做的嵌入模型比较，需要用到比较向量相似度，在知乎上看到了&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33164335&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;一篇文章&lt;/a&gt;，简单搬运过来做一些笔记和代码实践。首</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="python" scheme="https://cliccker.top/tags/python/"/>
    
    <category term="向量" scheme="https://cliccker.top/tags/%E5%90%91%E9%87%8F/"/>
    
    <category term="知识嵌入" scheme="https://cliccker.top/tags/%E7%9F%A5%E8%AF%86%E5%B5%8C%E5%85%A5/"/>
    
  </entry>
  
  <entry>
    <title>如何申请GitHub学生包</title>
    <link href="https://cliccker.top/post/e45b.html"/>
    <id>https://cliccker.top/post/e45b.html</id>
    <published>2020-07-22T10:16:00.000Z</published>
    <updated>2020-08-09T10:19:50.572Z</updated>
    
    <content type="html"><![CDATA[<p>事情是这样的，刚打开Pycharm出现了这样的一幕：</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722181810.png" alt="证书他又双叒过期了" style="zoom:50%;" /><p>遂想找个方法再续上一续，结果发现了Jetbrain其实开放了学生免费使用Pycharm开发，网址<a href="https://sales.jetbrains.com/hc/zh-cn/articles/207154369-%E5%AD%A6%E7%94%9F%E6%8E%88%E6%9D%83%E7%94%B3%E8%AF%B7%E6%96%B9%E5%BC%8F" target="_blank" rel="noopener">戳我</a></p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722181957.png" alt="JetBrain上的申请说明" style="zoom:67%;" /><p>这才意识到我不就是学生吗！遂准备申请，点进去看到了这样的选项：</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722182653.png" alt="又看到我们的老朋友Github" style="zoom:67%;" /><p>原来GitHub为学生准备了专门的开发包</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722191656.png" style="zoom:50%;" /><p>内容很丰富，都列举不完，<a href="https://education.github.com/pack" target="_blank" rel="noopener">戳我查看</a>，来一起看看如何申请这个包吧！</p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p>点击<code>Get the pack</code>，选择最左侧的<code>Get students benefits</code>。然后你会看到这样一个界面</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722190935.png" alt="第一步"></p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722190944.png" alt="批注 2020-07-22 185645" style="zoom:80%;" /><p>如何填写我都放在图片里了。</p><p>点击提交你会看到<img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722191150.png" alt="哇要等18天呐（其实应该不用那么久）" style="zoom: 67%;" /></p><p>好，到这里我们开始等待吧，如果成功了我就把这篇发在别的地方。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;事情是这样的，刚打开Pycharm出现了这样的一幕：&lt;/p&gt;
&lt;img src=&quot;https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722181810.png&quot; alt=&quot;证书他又双叒过期了&quot; style=&quot;zo</summary>
      
    
    
    
    <category term="技巧" scheme="https://cliccker.top/categories/%E6%8A%80%E5%B7%A7/"/>
    
    
    <category term="GitHub" scheme="https://cliccker.top/tags/GitHub/"/>
    
    <category term="白嫖" scheme="https://cliccker.top/tags/%E7%99%BD%E5%AB%96/"/>
    
  </entry>
  
  <entry>
    <title>如何加速Github访问速度</title>
    <link href="https://cliccker.top/post/d480.html"/>
    <id>https://cliccker.top/post/d480.html</id>
    <published>2020-07-22T07:46:00.000Z</published>
    <updated>2020-08-09T10:19:50.570Z</updated>
    
    <content type="html"><![CDATA[<p>最近比较烦时常抽风的GitHub，搞得我不能愉快的<del>克隆</del>借鉴代码，再加上之前一直在用的鸡场场长跑路了，所以尝试改hosts来实现。</p><h2 id="首先把hosts放出来"><a href="#首先把hosts放出来" class="headerlink" title="首先把hosts放出来"></a>首先把hosts放出来</h2><p>路径是</p><pre><code>C:\Windows\System32\drivers\etc\hosts</code></pre><p>你可以把它复制到桌面上，增加三行，再覆盖原文件。</p><pre><code>140.82.112.3                github.com185.199.108.153             assets-cdn.github.com199.232.69.194              github.global.ssl.fastly.net</code></pre><p>当然这只是我的配置，你可以拿去用但是我不知道能不能奏效。</p><p>如果效果不好，你可以自己在<a href="https://www.ipaddress.com/" target="_blank" rel="noopener">ipaddress</a>上查找对应域名的ip然后进行更改。</p><p>值得注意的是第二个加速域名有多个ip，任选一个即可。</p><h2 id="到这里还没完"><a href="#到这里还没完" class="headerlink" title="到这里还没完"></a>到这里还没完</h2><p>打开CMD，输入</p><pre><code>ipconfig /flushdns</code></pre><p>回车后执行刷新本地DNS缓存数据。</p><p>试试看访问我的<a href="https://github.com/Cliccker" target="_blank" rel="noopener">个人主页</a>，如果能快速加载出来就说明奏效了。</p><h2 id="但是"><a href="#但是" class="headerlink" title="但是"></a>但是</h2><p>访问速度是快了不少，下载还是龟速怎么办？</p><p>一种方法是在码云上新建一个仓库，然后把Repo搬过来下载，个人觉得这样做有点麻烦。</p><p>另一种方法比较推荐，就是利用文件代下载服务，如<a href="https://shrill-pond-3e81.hunsh.workers.dev/" target="_blank" rel="noopener">https://shrill-pond-3e81.hunsh.workers.dev/</a></p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200722162047.png" alt="加速下载"></p><p>感谢热衷于分享的程序⚪们！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近比较烦时常抽风的GitHub，搞得我不能愉快的&lt;del&gt;克隆&lt;/del&gt;借鉴代码，再加上之前一直在用的鸡场场长跑路了，所以尝试改hosts来实现。&lt;/p&gt;
&lt;h2 id=&quot;首先把hosts放出来&quot;&gt;&lt;a href=&quot;#首先把hosts放出来&quot; class=&quot;header</summary>
      
    
    
    
    <category term="技巧" scheme="https://cliccker.top/categories/%E6%8A%80%E5%B7%A7/"/>
    
    
    <category term="GitHub" scheme="https://cliccker.top/tags/GitHub/"/>
    
    <category term="加速" scheme="https://cliccker.top/tags/%E5%8A%A0%E9%80%9F/"/>
    
  </entry>
  
  <entry>
    <title>《算法设计与分析课程》笔记</title>
    <link href="https://cliccker.top/post/b315.html"/>
    <id>https://cliccker.top/post/b315.html</id>
    <published>2020-07-19T06:26:19.000Z</published>
    <updated>2020-08-09T10:19:50.573Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-1-算法的基本概念"><a href="#1-1-算法的基本概念" class="headerlink" title="1.1__算法的基本概念"></a>1.1__算法的基本概念</h2><p>算法的重要属性——<code>正确</code>和<code>高效</code></p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200719143345.png" alt="时间复杂度" style="zoom:50%;" /><h2 id="1-2-算法举例"><a href="#1-2-算法举例" class="headerlink" title="1.2__算法举例"></a>1.2__算法举例</h2><h3 id="局部高点"><a href="#局部高点" class="headerlink" title="局部高点"></a>局部高点</h3><p>定义：存在列表$A$，若$A[i-1] \leq A[i] \geq A[i+1]$，则称$A[i]$为局部高点。不是所有序列都有局部高点，这里限制边界条件$A[-1] = A[n]= -\infty$。</p><h4 id="简单算法："><a href="#简单算法：" class="headerlink" title="简单算法："></a>简单算法：</h4><p>找出数列中任意一个局部高点</p><p>逐个索引，比较前后元素之间的大小</p><ul><li>最好的情况：第一个元素就是局部高点 <code>1次</code></li><li>最坏的情况：最后一个元素才是局部高点<code>n次</code></li></ul><p><font color="#4590a3" size="4px">算法分析应考虑最坏的情况</font>，实际分析中应尽量忽略数据的分布</p><p>所以简单算法的效率事$O(n)$，函数是一个单调递增的线性函数</p><h4 id="更高效的算法："><a href="#更高效的算法：" class="headerlink" title="更高效的算法："></a>更高效的算法：</h4><p>考虑每次查找都缩小查找的范围</p><p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200719160534.png" alt="三种可能存在的情况"></p><p>从中间的元素开始查找，可以在一次比较后，缩小一半的搜索范围（二分法）</p><p>算法的效率为$O(log_2n)$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-1-算法的基本概念&quot;&gt;&lt;a href=&quot;#1-1-算法的基本概念&quot; class=&quot;headerlink&quot; title=&quot;1.1__算法的基本概念&quot;&gt;&lt;/a&gt;1.1__算法的基本概念&lt;/h2&gt;&lt;p&gt;算法的重要属性——&lt;code&gt;正确&lt;/code&gt;和&lt;code&gt;高</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="python" scheme="https://cliccker.top/tags/python/"/>
    
    <category term="算法" scheme="https://cliccker.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>如何将MarkDown笔记导出至知乎</title>
    <link href="https://cliccker.top/post/94fe.html"/>
    <id>https://cliccker.top/post/94fe.html</id>
    <published>2020-07-16T12:08:00.000Z</published>
    <updated>2020-08-09T10:19:50.567Z</updated>
    
    <content type="html"><![CDATA[<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 2019-11-26</span><span class="token keyword">import</span> re<span class="token keyword">import</span> sys<span class="token keyword">def</span> <span class="token function">replace</span><span class="token punctuation">(</span>file_name<span class="token punctuation">,</span> output_file_name<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        pattern1 <span class="token operator">=</span> r<span class="token string">"\$\$\n*([\s\S]*?)\n*\$\$"</span>        new_pattern1 <span class="token operator">=</span> r<span class="token string">'\n&lt;img src="https://www.zhihu.com/equation?tex=\1" alt="\1" class="ee_img tr_noresize" eeimg="1">\n'</span>        pattern2 <span class="token operator">=</span> r<span class="token string">"\$\n*(.*?)\n*\$"</span>        new_pattern2 <span class="token operator">=</span>r<span class="token string">'\n&lt;img src="https://www.zhihu.com/equation?tex=\1" alt="\1" class="ee_img tr_noresize" eeimg="1">\n'</span>        f <span class="token operator">=</span> open<span class="token punctuation">(</span>file_name<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>        f_output <span class="token operator">=</span> open<span class="token punctuation">(</span>output_file_name<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>        all_lines <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>        new_lines1 <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>pattern1<span class="token punctuation">,</span> new_pattern1<span class="token punctuation">,</span> all_lines<span class="token punctuation">)</span>        new_lines2 <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>pattern2<span class="token punctuation">,</span> new_pattern2<span class="token punctuation">,</span> new_lines1<span class="token punctuation">)</span>        f_output<span class="token punctuation">.</span>write<span class="token punctuation">(</span>new_lines2<span class="token punctuation">)</span>        f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        f_output<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    file_name <span class="token operator">=</span> <span class="token string">'original_version.md'</span>    file_name_pre <span class="token operator">=</span> file_name<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    output_file_name <span class="token operator">=</span> <span class="token string">"zhihu_version.md"</span>    replace<span class="token punctuation">(</span>file_name<span class="token punctuation">,</span> output_file_name<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Trans from {} to {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>file_name<span class="token punctuation">,</span> output_file_name<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>放在同一文件夹下，将markdown文件改名为’original_version.md’，运行就可以了</p>]]></content>
    
    
      
      
    <summary type="html">&lt;pre class=&quot; language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token comment&quot; spellcheck=&quot;true&quot;&gt;# 2019-11-26&lt;/span&gt;

&lt;span class=&quot;</summary>
      
    
    
    
    <category term="技巧" scheme="https://cliccker.top/categories/%E6%8A%80%E5%B7%A7/"/>
    
    
    <category term="MarkDown" scheme="https://cliccker.top/tags/MarkDown/"/>
    
  </entry>
  
  <entry>
    <title>读论文——On2Vec：基于嵌入的本体群体关系预测</title>
    <link href="https://cliccker.top/post/947f.html"/>
    <id>https://cliccker.top/post/947f.html</id>
    <published>2020-07-16T04:36:00.000Z</published>
    <updated>2020-08-09T10:19:50.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="读论文——On2Vec：基于嵌入的本体群体关系预测"><a href="#读论文——On2Vec：基于嵌入的本体群体关系预测" class="headerlink" title="读论文——On2Vec：基于嵌入的本体群体关系预测"></a>读论文——On2Vec：基于嵌入的本体群体关系预测</h1><p><em>原标题*：On2Vec: Embedding-based Relation Prediction for Ontology Population  *<a href="https://arxiv.org/abs/1809.02382" target="_blank" rel="noopener">来源</a></em> <a href="https://github.com/muhaochen/on2vec" target="_blank" rel="noopener"><em>代码</em></a></p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><h3 id="目标领域"><a href="#目标领域" class="headerlink" title="目标领域"></a>目标领域</h3><p>本体填充（Ontology population），指将原始信息（可以是非结构化、半结构化或者结构化的数据）转换为本体实例的过程。</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>现有的研究已经能将基于翻译的知识嵌入模式应用到实例层级的图谱中，实现较好的填充效果。相比于实例图谱，本体视图中的关系事实包含了更多复杂的语义关系，包括<code>可传递性</code>，<code>对称性</code>和<code>层次关系</code>。这种关系对于现有的嵌入模式来说过于复杂，且直接应用不可行。在TransE中我们用能量方程$S_{r}(\mathbf{s}, \mathbf{t})$去衡量一个三元组的可信度，函数值越小，代表描述就越准确。<br>$$<br>S_{r}(\mathbf{s}, \mathbf{t})=|\mathbf{s}+\mathbf{r}-\mathbf{t}|<br>$$<br>文章提出这种能量方程会导致如图所示的问题：</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/批注 2020-07-16 162227.png" alt="图1" title="图1" style="zoom:50%;" /><ul><li><p>Case1：$\boldsymbol {A,B,C}$是三个概念$A,B,C$的嵌入，假设<br>$$<br>\boldsymbol {A+r\approx B，B+r\approx C}<br>$$<br>即$r$是一种具有传递性的关系，按照传递性的原则应当有：<br>$$<br>\boldsymbol {A+r\approx C}<br>$$<br>然而事实上：<br>$$<br>\boldsymbol {A + r\ne C}<br>$$<br>这个结论很容易就能从图中观察出来。</p></li><li><p>Case2：$\boldsymbol {E,F}$是两个概念$E,F$的嵌入，假设：<br>$$<br>\boldsymbol {E+r\approx F}<br>$$<br>且$r$为对称性关系，则应有：</p><p>$$<br>\boldsymbol {F+r\approx E}<br>$$</p><p>然而事实上：<br>$$<br>\boldsymbol {F+r\ne E}<br>$$<br>这是因为向量$\boldsymbol r \ne 0$。</p></li></ul><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p>由两个模型组件组成的On2vec，包括：</p><ol><li>Component-specific Model （组件特定模型）将概念和关系编码嵌入低维空间，且不丢失相关属性；</li><li>Hierarchy Model  （层次模型）集中处理层级关系。</li></ol><h2 id="技术路线"><a href="#技术路线" class="headerlink" title="技术路线"></a>技术路线</h2><h3 id="符号及其含义"><a href="#符号及其含义" class="headerlink" title="符号及其含义"></a>符号及其含义</h3><table><thead><tr><th align="center">字符</th><th align="center">含义</th><th align="center">示例</th></tr></thead><tbody><tr><td align="center">$G(C,R）$</td><td align="center">一个图谱</td><td align="center"></td></tr><tr><td align="center">$C$</td><td align="center">一系列的概念</td><td align="center"></td></tr><tr><td align="center">$R$</td><td align="center">一系列语义关系</td><td align="center"></td></tr><tr><td align="center">$T=(s,r,t)$</td><td align="center">一个三元组</td><td align="center"></td></tr><tr><td align="center">$\boldsymbol s$</td><td align="center">vectors of source</td><td align="center"></td></tr><tr><td align="center">$\boldsymbol r$</td><td align="center">vectors of relation</td><td align="center"></td></tr><tr><td align="center">$\boldsymbol t$</td><td align="center">vectors of target</td><td align="center"></td></tr><tr><td align="center">$R_{tr}$</td><td align="center">传递关系</td><td align="center">如isConnectedTo</td></tr><tr><td align="center">$R_s$</td><td align="center">对称关系</td><td align="center">如isMarriedTo</td></tr><tr><td align="center">$R_h$</td><td align="center">层级关系</td><td align="center"></td></tr><tr><td align="center">$R_r$</td><td align="center">将粗概念划分为细概念的细化关系</td><td align="center">如hasChild</td></tr><tr><td align="center">$R_c$</td><td align="center">将细概念划分到粗概念的强制关系</td><td align="center">如isLocatedIn</td></tr><tr><td align="center">$R_o$</td><td align="center">其他关系</td><td align="center"></td></tr></tbody></table><p>我可以用不同的数学表达式展示各种关系，如传递关系$R_{tr}$</p><p>$$<br>given: r \in R_{tr} \ c_1,c_2,c_3 \in G \ if:(c_1,r,c_2),(c_2,r,c_3) \in G<br>\ then : (c_1,r,c_3) \in G<br>$$</p><h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><h4 id="Component-specific-Model"><a href="#Component-specific-Model" class="headerlink" title="Component-specific Model"></a>Component-specific Model</h4><h5 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h5><p>这个名字不知道怎么翻译合适。暂时称为<code>特定组件模型</code>。</p><p>文章在这里认为，<code>关系投影函数</code>$f_r$在投影时，就已经把复杂关系中的概念放在了有冲突的位置上。为了解决这个两个问题，CSM提出了用两个不相同的$f_r$，去区别同一概念在不同三元组中的嵌入。仔细解释一下就是，对于同一个概念，在不同三元组中的<code>组分</code>会不同，因此在衡量一个三元组的<code>可信度</code>时，需要对该三元组中概念的<code>嵌入</code>做一定的调整才能解决这些冲突。据此文章提出了一种新的衡量可信度方程$S_{d}(T)$</p><p>$$<br>S_{d}(T)=| f_{1, r}(\mathbf{s})+\mathbf{r}-f_{2, r}(\mathbf{t}) \mid<br>$$</p><p>式中的$f_{1,r}(x),f_{2,r}(x)$是作用于头尾概念的不同<code>组分投影函数</code>，后文提到了这两个函数实际就是两个$k \times k $的矩阵，这两个矩阵的形式由概念在<strong>不同三元组中的组分</strong>决定。</p><blockquote><p>The forms of $f_{1,r}$and $f_{2,r}$ are decided particularly by the techniques to differentiate the concept encoding under different contexts of relations.  </p></blockquote><p>文章还提到$S_{d}$在消除冲突的同时，还能用来实现挖掘本体中隐藏的关系。现在我们在处理Case 1时，就能把在不同三元组中的$B$放在不同的位置。对于Case 2 ，可以让$E，F$互换位置。</p><h5 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h5><p>CSM的目标是最小化能量函数，也就是增加所有三元组的可信度。和别的模型选择<code>实体</code>做负面样本不一样的是，CSM选择<code>关系</code>来做负面的样本，由此提出loss：</p><p>$$<br>\begin{aligned}<br>S_{\mathrm{CSM}}(G)=&amp; \sum_{(s, r, t) \in G}\left[\left|f_{1, r}(\mathbf{s})+\mathbf{r}-f_{2, r}(\mathbf{t})\right|\right.\left.-\left|f_{1, r}(\mathbf{s})+\mathbf{r}^{\prime}-f_{2, r}(\mathbf{t})\right|+\gamma_{1}\right]_{+}<br>\end{aligned}<br>$$</p><p>式中的$r’$是一个<code>并不能连接</code>$s$和$t$的<code>随机选取</code>的关系，$\gamma_{1}$是一个正矩阵。学习目的是为了得到更小的$S_{CSM}$</p><h4 id="Hierarchy-Model"><a href="#Hierarchy-Model" class="headerlink" title="Hierarchy Model"></a>Hierarchy Model</h4><h5 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h5><p>对于层次关系，这篇文章提到了我读的<a href="https://zhuanlan.zhihu.com/p/156791237" target="_blank" rel="noopener">另一篇论文</a>中对层级关系的处理方法，就是让他们在向量空间上尽量聚集。如图</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200717125248.png" title ="图2 上一篇文章提到的分组技术" style="zoom: 67%;" /><p>文章提出较为精细的概念，比如”person”，可以参与多个关系事实，这就导致了在嵌入时一个关系事实容易受到其他关系事实的影响，降低了三元组的可信度。HM要做的是将每一个精细概念的嵌入更加紧密的<code>融合</code>在一起。为了做到这一点，文章提出了一种<code>精炼</code>的操作，如图：</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200717151516.png" alt="图3 ‘精炼’操作" style="zoom:67%;" /><p>这一步骤是为了让所有<code>直接</code>相关的概念聚集，形成一个个<code>群体</code>。<br>举例说明：</p><ul><li><p>设$(c_1,isA,c_2),(c_2,isA,c_3),(c_4,isA,c_3),(c_5,isA,c_3) \in G$</p></li><li><p>则有$\sigma(c_3,isA) = [c_2,c_4,c_5]$</p></li><li><p>虽然$(c_1,isA,c_2),(c_2,isA,c_3)$得到$(c_1,isA,c_3)$，但是这里认为$c_1$并不与$c_3$直接相关，故$c_1 \notin\sigma(c_3,isA)$</p></li></ul><p>这样做使得<code>群体</code>的数量大大增加，<code>群体</code>之间的联系减少，减少了关系事实之间的干扰。同时也要考虑，在某一群体中处于中心的概念嵌入，在另一群体中将处于边缘，所以也需要对其嵌入做适当调整。</p><p>据此列出能量方程：<br>$$<br>\begin{aligned}<br>S_{h m}(G) &amp;=\sum_{r \in R_{r}} \sum_{s \in C} \sum_{t \in \sigma(s, r)} \omega\left(f_{1, r}(\mathbf{s})+\mathbf{r}, f_{2, r}(\mathbf{t})\right) \<br>&amp;+\sum_{r \in R_{c}} \sum_{t \in C} \sum_{s \in \sigma(t, r)} \omega\left(f_{2, r}(\mathbf{t})-\mathbf{r}, f_{1, r}(\mathbf{s})\right)<br>\end{aligned}<br>$$<br>$f_{1,r}和f_{2,r}$就是上一节写到的调整函数。$\omega (x)$是用来计算两个向量的相似度的单调递增的函数，算法中直接计算两个向量的余弦距离，余弦距离越小，余弦相似度越高。第一行计算的是所有<code>直接细化关系</code>的三元组的偏差，第二行计算的是所有<code>直接强制关系</code>的三元组的偏差，相加得到整个图谱的偏差。</p><h5 id="优化-1"><a href="#优化-1" class="headerlink" title="优化"></a>优化</h5><p>$$<br>\begin{aligned}<br>S_{\mathrm{HM}}(G) &amp;=\sum_{r \in R_{r}} \sum_{s \in C} \sum_{t \in \sigma(s, r) \wedge \ t^{\prime} \notin \sigma(s, r)} S_{h r} \<br>&amp;+\sum_{r \in R_{c}} \sum_{t \in C} \sum_{s \in \sigma(t, r) \wedge \ s^{\prime} \notin \sigma(t, r)} S_{h c}<br>\end{aligned}<br>$$</p><p>$s’$和$t’$在这里做负面样本，其中</p><p>$$<br>S_{h r}=\left[\omega\left(f_{1, r}(\mathbf{s})+\mathbf{r}, f_{2, r}(\mathbf{t})\right)-\omega\left(f_{1, r}(\mathbf{s})+\mathbf{r}, f_{2, r}\left(\mathbf{t}^{\prime}\right)\right)+\gamma_{2}\right]_{+}<br>$$</p><p>$$<br>S_{h c}=\left[\omega\left(f_{2, r}(\mathbf{t})-\mathbf{r}, f_{1, r}(\mathbf{s})\right)-\omega\left(f_{2, r}(\mathbf{t})-\mathbf{r}, f_{1, r}\left(\mathbf{s}^{\prime}\right)\right)+\gamma_{2}\right]_{+}<br>$$</p><h3 id="学习流程"><a href="#学习流程" class="headerlink" title="学习流程"></a>学习流程</h3><p>学习的目标是联合损失最小，联合损失的表达式如下<br>$$<br>J(\theta)=S_{\mathrm{CSM}}+\alpha_{1} S_{\mathrm{HM}}+\alpha_{2} S_{\mathrm{N}}<br>$$</p><ul><li>$\theta$ 是包括嵌入向量和投影矩阵的一系列的参数的集合</li><li>$\alpha_{1} S_{\mathrm{HM}}$中的$\alpha_1$用来调整两个模型的权重</li><li>$\alpha_1S_N$中的$\alpha_1$是一个大于零小于等一的数，$S_N$用来对嵌入和投影施加约束，防止出现向量趋向于无限大的情况。其形式如下</li></ul><img src="C:\Users\76084\Desktop\20200720152243.png" alt="$S_N$" style="zoom: 50%;" /><p>最后给出了具体算法流程如图</p><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200717171122.png" alt="图4 算法" style="zoom:67%;" /><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><img src="https://my-picbed.oss-cn-hangzhou.aliyuncs.com/img/20200717172910.png" alt="图5 数据集" style="zoom:67%;" /><p>在实验中，这个模型被用来做<code>关系预测</code>和<code>关系识别</code>。关系预测准确率在90%左右，关系识别的准确率波动较大，最高有98%，最低73%</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>这篇论文看的我蛮痛苦的，因为里面好多用词都不是那么精准，而且也没有合适的图例表达算法逻辑，和之前发出来那篇差了好多，许多地方并不能说服我，有时间再看看他的源码吧。果然好论文一定是容易读的论文！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;读论文——On2Vec：基于嵌入的本体群体关系预测&quot;&gt;&lt;a href=&quot;#读论文——On2Vec：基于嵌入的本体群体关系预测&quot; class=&quot;headerlink&quot; title=&quot;读论文——On2Vec：基于嵌入的本体群体关系预测&quot;&gt;&lt;/a&gt;读论文——On2Vec</summary>
      
    
    
    
    <category term="学习" scheme="https://cliccker.top/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="知识嵌入" scheme="https://cliccker.top/tags/%E7%9F%A5%E8%AF%86%E5%B5%8C%E5%85%A5/"/>
    
    <category term="本体" scheme="https://cliccker.top/tags/%E6%9C%AC%E4%BD%93/"/>
    
  </entry>
  
</feed>
